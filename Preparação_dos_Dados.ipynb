{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7ktyOwJyEjwd",
        "P2H0eASGAVwP",
        "BpHzXD_0F2yT",
        "zKu3fVDfHw1B",
        "_9yXQ37IEB0r",
        "rL3DayLiEnm2",
        "WI4GnhYdF2b2",
        "zEpIHqraO91h",
        "vJy1pey4UwdQ",
        "hvPJi35S2AlN"
      ],
      "authorship_tag": "ABX9TyP6G2FvuSiZykLZo9aCnju+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorhss21/Estudos_CD/blob/main/Prepara%C3%A7%C3%A3o_dos_Dados.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preparação dos dados**"
      ],
      "metadata": {
        "id": "qoTG3fEtB4tT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Neste notebook, iremos abordar técnicas para:\n",
        ">\n",
        "> * **Escalonamento dos dados** (Padronização, Min-Max Scaler, Robust Scaler, etc.)\n",
        ">\n",
        "> * **Transformação de variáveis categóricas** (Label Encoding, Target Enconding, OneHot Encoding, etc.)\n",
        ">\n",
        "> * **Seleção de features** (Métodos Incorporados, Filtros, Empacotamento/Wrapper, etc.)\n",
        ">\n",
        "> * **Redução de dimensionalidade** (PCA, LDA, etc.)"
      ],
      "metadata": {
        "id": "YnsjkdyRDEiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **I. Escalonamento dos Dados** (Data Scaling)\n",
        "\n",
        "O escalonamento **ajusta o intervalo dos dados** numéricos **sem alterar a forma de sua distribuição**."
      ],
      "metadata": {
        "id": "FIcYj8mAACIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **1. Padronização (StandardScaler)**"
      ],
      "metadata": {
        "id": "P2H0eASGAVwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **O que é?**\n",
        "    *   **Motivação:** Muitos algoritmos de ML (especialmente os **baseados em distância** como **KNN**, **SVM**, **K-Means**, e os que **usam gradiente descendente** como **Regressão Linear/Logística**, **Redes Neurais**) são **sensíveis à escala das features**. Features com *maiores magnitudes* podem *dominar indevidamente* o processo de aprendizado ou otimização.\n",
        "    *   **Proposta:** Transformar os dados para que tenham **média zero (μ=0)** e **desvio padrão um (σ=1)**. **Não limita os valores a um intervalo específico**.\n",
        "*   **Qual o seu funcionamento?**\n",
        "    *   **Pressupostos:** Assume que os **dados seguem** (ou se aproximam) de uma **distribuição Gaussiana**, onde média e desvio padrão são estatísticas significativas. No entanto, é frequentemente **aplicado mesmo quando essa premissa não é estritamente verdadeira**.\n",
        "    *   **Lógica:** Para cada valor `x` em uma feature, calcula-se o valor padronizado `z` usando a fórmula: `z = (x - μ) / σ`, onde `μ` é a média da feature e `σ` é o desvio padrão da feature, calculados **apenas** no conjunto de treino.\n",
        "*   **Como utilizá-lo? (Python com `scikit-learn`)**"
      ],
      "metadata": {
        "id": "wB5hLIPhAPt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dados de exemplo (Idade, Salário)\n",
        "data = {'Idade': [25, 30, 35, 40, 45, 50, 55, 60],\n",
        "        'Salario': [50000, 60000, 75000, 90000, 110000, 130000, 155000, 180000]}\n",
        "df = pd.DataFrame(data)\n",
        "features = ['Idade', 'Salario']\n",
        "\n",
        "# Simular divisão treino/teste\n",
        "X_train, X_test = train_test_split(df[features], test_size=0.25, random_state=42)\n",
        "\n",
        "# 1. Instanciar o scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# 2. Ajustar SOMENTE nos dados de treino e transformar treino\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# 3. Transformar os dados de teste com o scaler AJUSTADO no treino\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Dados de Treino Originais:\\n\", X_train)\n",
        "print(\"\\nDados de Treino Escalados (Média ~0, DP ~1):\\n\", X_train_scaled)\n",
        "print(\"\\nMédia das features escaladas (Treino):\", X_train_scaled.mean(axis=0))\n",
        "print(\"DP das features escaladas (Treino):\", X_train_scaled.std(axis=0))\n",
        "print(\"\\nDados de Teste Escalados:\\n\", X_test_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNg1v4hvAdWI",
        "outputId": "e53215f3-61c4-4935-8128-d5121c65f684"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados de Treino Originais:\n",
            "    Idade  Salario\n",
            "0     25    50000\n",
            "7     60   180000\n",
            "2     35    75000\n",
            "4     45   110000\n",
            "3     40    90000\n",
            "6     55   155000\n",
            "\n",
            "Dados de Treino Escalados (Média ~0, DP ~1):\n",
            " [[-1.55563492 -1.33333333]\n",
            " [ 1.41421356  1.55555556]\n",
            " [-0.70710678 -0.77777778]\n",
            " [ 0.14142136  0.        ]\n",
            " [-0.28284271 -0.44444444]\n",
            " [ 0.98994949  1.        ]]\n",
            "\n",
            "Média das features escaladas (Treino): [-2.03540888e-16  1.85037171e-17]\n",
            "DP das features escaladas (Treino): [1. 1.]\n",
            "\n",
            "Dados de Teste Escalados:\n",
            " [[-1.13137085 -1.11111111]\n",
            " [ 0.56568542  0.44444444]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Quais suas vantagens e limitações?**\n",
        "    *   **Vantagens:** Padrão para muitos algoritmos. **Menos sensível a outliers do que Min-Max** (mas ainda afetado, pois usa média/DP). **Preserva a forma da distribuição original** (apenas translada e redimensiona).\n",
        "    *   **Limitações:** Não garante um intervalo fixo para os dados. A média e o desvio padrão são sensíveis a outliers, o que pode distorcer o escalonamento.\n",
        "    *   **Precauções:** **Fundamental:** Ajustar (`fit`) o scaler ***apenas* no conjunto de treino** para evitar vazamento de dados (data leakage) do conjunto de teste para o processo de pré-processamento. Aplicar `transform` em ambos, treino e teste.\n",
        "*   **Quais as principais dicas práticas?**\n",
        "    *   É um bom ponto de partida \"padrão\" para a maioria dos algoritmos sensíveis à escala.\n",
        "    *   Se seus dados tiverem **muitos outliers** significativos, **considere o `RobustScaler`** primeiro.\n",
        "    *   Verifique visualmente (histogramas) a distribuição antes e depois para entender o efeito."
      ],
      "metadata": {
        "id": "DKDQQucHAj66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Normalização (MinMaxScaler)**"
      ],
      "metadata": {
        "id": "BpHzXD_0F2yT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **O que é?**\n",
        "    *   **Motivação:** Alguns algoritmos ou cenários exigem que os **dados estejam estritamente dentro de um intervalo específico**, geralmente **[0, 1] (ou [-1, 1])**. Exemplos incluem processamento de imagens (intensidade de pixels) ou algumas ativações em redes neurais.\n",
        "    *   **Proposta:** Redimensionar os dados para que fiquem **contidos em um intervalo definido**, preservando a relação entre os valores.\n",
        "*   **Qual o seu funcionamento?**\n",
        "    *   **Pressupostos:** **Não faz fortes suposições** sobre a distribuição, mas é **altamente influenciado pelos valores mínimo e máximo**.\n",
        "    *   **Lógica:** Para cada valor `x` em uma feature, calcula-se o valor normalizado `x_scaled` usando a fórmula: `x_scaled = (x - min) / (max - min)`, onde `min` e `max` são os valores mínimo e máximo da feature, calculados **apenas** no conjunto de treino. Para um intervalo [a, b], a fórmula é ajustada.\n",
        "*   **Como utilizá-lo? (Python com `scikit-learn`)**\n",
        "   "
      ],
      "metadata": {
        "id": "kwCLlcck_7Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# ... (usando os mesmos dados e divisão treino/teste de antes) ...\n",
        "\n",
        "# 1. Instanciar o scaler (intervalo padrão [0, 1])\n",
        "min_max_scaler = MinMaxScaler()\n",
        "\n",
        "# 2. Ajustar SOMENTE nos dados de treino e transformar treino\n",
        "X_train_normalized = min_max_scaler.fit_transform(X_train)\n",
        "\n",
        "# 3. Transformar os dados de teste\n",
        "X_test_normalized = min_max_scaler.transform(X_test)\n",
        "\n",
        "print(\"Dados de Treino Normalizados ([0, 1]):\\n\", X_train_normalized)\n",
        "print(\"\\nMínimo das features normalizadas (Treino):\", X_train_normalized.min(axis=0))\n",
        "print(\"Máximo das features normalizadas (Treino):\", X_train_normalized.max(axis=0))\n",
        "print(\"\\nDados de Teste Normalizados:\\n\", X_test_normalized)\n",
        "# Note que os valores no teste podem cair fora de [0, 1] se forem menores/maiores que min/max do treino\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6OJ4lr6GzJ6",
        "outputId": "33ee171c-d07e-4b79-bd83-6e273fdeb3c0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados de Treino Normalizados ([0, 1]):\n",
            " [[0.         0.        ]\n",
            " [1.         1.        ]\n",
            " [0.28571429 0.19230769]\n",
            " [0.57142857 0.46153846]\n",
            " [0.42857143 0.30769231]\n",
            " [0.85714286 0.80769231]]\n",
            "\n",
            "Mínimo das features normalizadas (Treino): [0. 0.]\n",
            "Máximo das features normalizadas (Treino): [1. 1.]\n",
            "\n",
            "Dados de Teste Normalizados:\n",
            " [[0.14285714 0.07692308]\n",
            " [0.71428571 0.61538462]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Quais suas vantagens e limitações?**\n",
        "    *   **Vantagens:** Garante um **intervalo fixo**, útil para algoritmos específicos. Preserva relações de ordem e distribuições (comprime/expande o eixo).\n",
        "    *   **Limitações:** **Extremamente sensível a outliers.** Um único valor muito alto ou baixo pode comprimir todo o restante dos dados em um intervalo muito pequeno, perdendo granularidade.\n",
        "    *   **Precauções:** Trate ou **remova outliers** *antes* de usar `MinMaxScaler`. Ajuste apenas no treino.\n",
        "*   **Quais as principais dicas práticas?**\n",
        "    *   Use principalmente quando o algoritmo *exige* dados em um intervalo específico (ex: intensidade de pixels 0-255 para [0,1]).\n",
        "    *   Se houver outliers, `MinMaxScaler` provavelmente não é a melhor escolha. Explore `RobustScaler` ou trate os outliers.\n",
        "\n"
      ],
      "metadata": {
        "id": "4MCQNsxPGuOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> É possível utilizar o Min-Max Scaler para gerar um intervalo fixo entre [-1, 1]. Para isso, basta modificar a fórmula da seguinte forma:\n",
        "\n",
        "$$\n",
        "X_{scaled} = \\frac{X - Min}{Max - Min} \\cdot (Max - min) + (Min)\n",
        "$$\n",
        "\n",
        "Na prática, podemos implementar isso facilmente utilizando o Scikit Learn da seguinte forma:"
      ],
      "metadata": {
        "id": "aeWMIsXeH0g2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# ... (usando os mesmos dados e divisão treino/teste de antes) ...\n",
        "\n",
        "# 1. Instanciar o scaler (intervalo [-1, 1])\n",
        "min_max_scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "\n",
        "# 2. Ajustar SOMENTE nos dados de treino e transformar treino\n",
        "X_train_normalized = min_max_scaler.fit_transform(X_train)\n",
        "\n",
        "# 3. Transformar os dados de teste\n",
        "X_test_normalized = min_max_scaler.transform(X_test)\n",
        "\n",
        "print(\"Dados de Treino Normalizados ([-1, 1]):\\n\", X_train_normalized)\n",
        "print(\"\\nMínimo das features normalizadas (Treino):\", X_train_normalized.min(axis=0))\n",
        "print(\"Máximo das features normalizadas (Treino):\", X_train_normalized.max(axis=0))\n",
        "print(\"\\nDados de Teste Normalizados:\\n\", X_test_normalized)\n",
        "# Note que os valores no teste podem cair fora de [-1, 1] se forem menores/maiores que min/max do treino"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8suCMaQ8I0A_",
        "outputId": "95332fe0-4106-4f27-c33b-a1d9e6ae7786"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados de Treino Normalizados ([-1, 1]):\n",
            " [[-1.         -1.        ]\n",
            " [ 1.          1.        ]\n",
            " [-0.42857143 -0.61538462]\n",
            " [ 0.14285714 -0.07692308]\n",
            " [-0.14285714 -0.38461538]\n",
            " [ 0.71428571  0.61538462]]\n",
            "\n",
            "Mínimo das features normalizadas (Treino): [-1. -1.]\n",
            "Máximo das features normalizadas (Treino): [1. 1.]\n",
            "\n",
            "Dados de Teste Normalizados:\n",
            " [[-0.71428571 -0.84615385]\n",
            " [ 0.42857143  0.23076923]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. Robust Scaler**"
      ],
      "metadata": {
        "id": "zKu3fVDfHw1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **O que é?**\n",
        "    *   **Motivação:** A Padronização e a Normalização são sensíveis a outliers. Se o conjunto de dados contém valores extremos significativos, a média/DP (StandardScaler) ou min/max (MinMaxScaler) serão distorcidos.\n",
        "    *   **Proposta:** Escalonar os dados usando estatísticas que são **robustas a outliers**: a **mediana** e o **intervalo interquartil** (IQR - Interquartile Range).\n",
        "*   **Qual o seu funcionamento?**\n",
        "    *   **Pressupostos:** **Não assume uma distribuição específica** e é projetado para **lidar com outliers**.\n",
        "    *   **Lógica:** Para cada valor `x` em uma feature, calcula-se o valor escalado `x_scaled` usando a fórmula: `x_scaled = (x - Q2) / (Q3 - Q1)`, onde `Q1` é o primeiro quartil (percentil 25), `Q2` é a mediana (percentil 50) e `Q3` é o terceiro quartil (percentil 75). `Q3 - Q1` é o IQR. **Mediana e IQR são muito menos afetados por valores extremos.**\n",
        "*   **Como utilizá-lo? (Python com `scikit-learn`)**\n",
        "    "
      ],
      "metadata": {
        "id": "FVoU2CjCHuFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "# ... (usando os mesmos dados e divisão treino/teste) ...\n",
        "\n",
        "# Adicionar um outlier para demonstração\n",
        "X_train_outlier = X_train.copy()\n",
        "X_train_outlier.loc[len(X_train_outlier)] = {'Idade': 38, 'Salario': 1000000} # Outlier no salário\n",
        "\n",
        "# 1. Instanciar o scaler\n",
        "robust_scaler = RobustScaler()\n",
        "\n",
        "# 2. Ajustar SOMENTE nos dados de treino (com outlier) e transformar treino\n",
        "X_train_robust_scaled = robust_scaler.fit_transform(X_train_outlier)\n",
        "\n",
        "# 3. Transformar os dados de teste (sem outlier aqui, mas usar o mesmo scaler)\n",
        "X_test_robust_scaled = robust_scaler.transform(X_test) # X_test original\n",
        "\n",
        "print(\"Dados de Treino (com outlier) Robust Scaled:\\n\", X_train_robust_scaled)\n",
        "# Note que o escalonamento é menos afetado pelo outlier comparado ao StandardScaler/MinMaxScaler\n",
        "print(\"\\nDados de Teste Robust Scaled:\\n\", X_test_robust_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo5F4AeqJPDa",
        "outputId": "6726135e-9908-4e07-b759-4d6ced3fb3aa"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados de Treino (com outlier) Robust Scaled:\n",
            " [[-1.75       -0.59701493]\n",
            " [ 2.625       0.95522388]\n",
            " [-0.5        -0.29850746]\n",
            " [ 0.75        0.11940299]\n",
            " [ 0.125      -0.11940299]\n",
            " [-0.125      10.74626866]]\n",
            "\n",
            "Dados de Teste Robust Scaled:\n",
            " [[-1.125      -0.47761194]\n",
            " [ 1.375       0.35820896]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Quais suas vantagens e limitações?**\n",
        "    *   **Vantagens:** Robusto a outliers. **Os dados escalados não são tão distorcidos por valores extremos.**\n",
        "    *   **Limitações:** Não centraliza a média em 0 ou o desvio padrão em 1 necessariamente. O intervalo resultante não é fixo como no `MinMaxScaler`.\n",
        "    *   **Precauções:** Ajuste apenas no treino.\n",
        "*   **Quais as principais dicas práticas?**\n",
        "    *   **Escolha padrão quando há suspeita ou confirmação de outliers significativos nos dados numéricos.**\n",
        "    *   Pode ser combinado com detecção/tratamento de outliers, mas é útil quando se deseja manter os outliers (ou não se tem certeza se são erros ou valores extremos válidos)."
      ],
      "metadata": {
        "id": "z6ywuSfZJMli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **II. Transformação de Variáveis Categóricas**\n",
        "\n",
        "Modelos de ML geralmente exigem input numérico. Variáveis categóricas (strings ou categorias numéricas sem ordem intrínseca) precisam ser convertidas."
      ],
      "metadata": {
        "id": "_9yXQ37IEB0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Label Encoding**"
      ],
      "metadata": {
        "id": "bV-0iVx-Xg1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **O que é?**\n",
        "    *   **Motivação:** A forma mais simples de converter categorias em números.\n",
        "    *   **Proposta:** Atribuir um número inteiro único (0, 1, 2, ...) a cada categoria distinta em uma feature.\n",
        "*   **Qual o seu funcionamento?**\n",
        "    *   **Pressupostos:** O algoritmo que receberá a feature *não* interpretará os números como tendo uma relação ordinal (ex: 2 > 1 > 0), a menos que essa relação realmente exista (feature ordinal).\n",
        "    *   **Lógica:** Mapeamento direto: 'Vermelho' -> 0, 'Verde' -> 1, 'Azul' -> 2. A ordem é geralmente arbitrária (alfabética).\n",
        "*   **Como utilizá-lo? (Python com `scikit-learn`)**\n",
        "    ```python\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    import pandas as pd\n",
        "\n",
        "    data = {'Cor': ['Vermelho', 'Verde', 'Azul', 'Verde', 'Vermelho']}\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # 1. Instanciar o encoder\n",
        "    label_encoder = LabelEncoder()\n",
        "\n",
        "    # 2. Ajustar e transformar (geralmente no dataset inteiro ou coluna específica)\n",
        "    df['Cor_Encoded'] = label_encoder.fit_transform(df['Cor'])\n",
        "\n",
        "    print(\"Label Encoding:\\n\", df)\n",
        "    print(\"\\nClasses mapeadas:\", label_encoder.classes_)\n",
        "    ```\n",
        "*   **Quais suas vantagens e limitações?**\n",
        "    *   **Vantagens:** Simples, rápido, eficiente em termos de memória (1 coluna).\n",
        "    *   **Limitações:** **Introduz uma relação ordinal artificial** que não existe na maioria das variáveis categóricas nominais. Modelos lineares, SVMs, KNNs, Redes Neurais podem interpretar erroneamente que 'Azul' (2) é \"maior\" ou \"mais importante\" que 'Verde' (1).\n",
        "    *   **Precauções:** **Geralmente inadequado para features de entrada (X)** em modelos não baseados em árvores. Use com cautela.\n",
        "*   **Quais as principais dicas práticas?**\n",
        "    *   **Ideal para a variável alvo (y) em problemas de classificação.** Muitos frameworks lidam bem com targets numéricos.\n",
        "    *   **Pode ser usado para features ordinais** onde a ordem dos números faz sentido (ex: 'Baixo'=0, 'Médio'=1, 'Alto'=2).\n",
        "    *   **Evite para features nominais de entrada** na maioria dos modelos. Use One-Hot Encoding ou outros métodos. Tree-based models (Decision Trees, Random Forest, Gradient Boosting) *podem* lidar bem com Label Encoding porque eles particionam o espaço, mas One-Hot é frequentemente mais seguro.\n"
      ],
      "metadata": {
        "id": "n-hzVfylVaTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. One-Hot Encoding (OHE)**\n"
      ],
      "metadata": {
        "id": "a6Wd7JQ4XmHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   **O que é?**\n",
        "    *   **Motivação:** Representar variáveis categóricas nominais numericamente sem introduzir uma ordem artificial.\n",
        "    *   **Proposta:** Criar novas colunas binárias (0 ou 1) para cada categoria presente na feature original.\n",
        "*   **Qual o seu funcionamento?**\n",
        "    *   **Pressupostos:** As categorias são independentes e não possuem relação ordinal.\n",
        "    *   **Lógica:** Se uma feature 'Cor' tem categorias ['Vermelho', 'Verde', 'Azul'], OHE cria 3 novas colunas: 'Cor_Vermelho', 'Cor_Verde', 'Cor_Azul'. Para uma linha onde a cor era 'Verde', a linha terá `Cor_Vermelho=0`, `Cor_Verde=1`, `Cor_Azul=0`.\n",
        "*   **Como utilizá-lo? (Python com `scikit-learn` ou `pandas`)**\n",
        "    ```python\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    import pandas as pd\n",
        "\n",
        "    data = {'Cor': ['Vermelho', 'Verde', 'Azul', 'Verde', 'Vermelho'],\n",
        "            'Tamanho': ['P', 'M', 'G', 'M', 'P']}\n",
        "    df = pd.DataFrame(data)\n",
        "    X = df[['Cor', 'Tamanho']] # Features para encodar\n",
        "\n",
        "    # Usando Scikit-learn (preferível em pipelines)\n",
        "    # sparse=False retorna array numpy, handle_unknown='ignore' evita erro se categoria nova aparecer no teste\n",
        "    # drop='first' remove a primeira categoria de cada feature para evitar multicolinearidade (N-1 colunas)\n",
        "    ohe_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first')\n",
        "\n",
        "    # Ajustar no TREINO, transformar TREINO e TESTE\n",
        "    # Simulando treino/teste\n",
        "    X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
        "\n",
        "    ohe_encoder.fit(X_train) # Ajusta com as categorias do treino\n",
        "    X_train_ohe = ohe_encoder.transform(X_train)\n",
        "    X_test_ohe = ohe_encoder.transform(X_test) # Transforma teste\n",
        "\n",
        "    # Obter nomes das novas colunas\n",
        "    feature_names = ohe_encoder.get_feature_names_out(['Cor', 'Tamanho'])\n",
        "    X_train_ohe_df = pd.DataFrame(X_train_ohe, columns=feature_names, index=X_train.index)\n",
        "    X_test_ohe_df = pd.DataFrame(X_test_ohe, columns=feature_names, index=X_test.index)\n",
        "\n",
        "    print(\"One-Hot Encoding (Scikit-learn, drop='first'):\\n\", X_train_ohe_df)\n",
        "\n",
        "    # Alternativa com Pandas (mais simples para análise exploratória)\n",
        "    df_ohe_pd = pd.get_dummies(df, columns=['Cor', 'Tamanho'], drop_first=True)\n",
        "    print(\"\\nOne-Hot Encoding (Pandas, drop_first=True):\\n\", df_ohe_pd)\n",
        "    ```\n",
        "*   **Quais suas vantagens e limitações?**\n",
        "    *   **Vantagens:** Representação não ambígua para categorias nominais. Funciona bem com a maioria dos algoritmos. Evita a armadilha da ordem artificial do Label Encoding.\n",
        "    *   **Limitações:** **Aumenta significativamente a dimensionalidade** se a variável categórica tiver muitas categorias (alta cardinalidade), levando à \"maldição da dimensionalidade\" e potentially a esparsidade. Pode introduzir multicolinearidade se não usar `drop='first'` (a soma das colunas OHE para uma feature é sempre 1).\n",
        "    *   **Precauções:** Cuidado com features de alta cardinalidade. Considere agrupar categorias raras em 'Outros' antes do OHE, ou usar outros métodos (Target Encoding, Embedding Layers). Use `handle_unknown='ignore'` ou similar para lidar com categorias não vistas no treino. `drop='first'` é recomendado para modelos lineares.\n",
        "*   **Quais as principais dicas práticas?**\n",
        "    *   **Método padrão para features nominais de baixa/média cardinalidade.**\n",
        "    *   Sempre use `handle_unknown='ignore'` (ou equivalente) em produção ou ao lidar com dados de teste/novos.\n",
        "    *   Para alta cardinalidade (>50-100 categorias, dependendo do dataset): agrupe categorias raras, use Target Encoding, ou Feature Hashing.\n",
        "    *   Para modelos de árvore, `drop='first'` é menos crítico, mas ainda pode ajudar. Para modelos lineares, é importante."
      ],
      "metadata": {
        "id": "mC_z6FR7Vcx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. Target Encoding (Mean Encoding)**\n"
      ],
      "metadata": {
        "id": "m43B9XbDXomi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   **O que é?**\n",
        "    *   **Motivação:** Capturar informação preditiva da variável alvo (target) diretamente na codificação da feature categórica, **especialmente útil para alta cardinalidade** e em competições (como Kaggle). Evita a explosão dimensional do OHE.\n",
        "    *   **Proposta:** Substituir cada categoria pelo valor médio (ou outra agregação) da variável alvo para as amostras que pertencem a essa categoria.\n",
        "*   **Qual o seu funcionamento?**\n",
        "    *   **Pressupostos:** A média do target para uma categoria é uma representação útil de seu poder preditivo. Requer cuidado extremo para evitar vazamento de dados (target leakage).\n",
        "    *   **Lógica:** Para uma categoria 'C' e um target 'y' (numérico ou binário 0/1), o valor codificado para 'C' é `mean(y | feature == 'C')`. Para evitar overfitting e leakage, esse cálculo **NÃO** deve usar o valor do target da própria linha que está sendo codificada. Estratégias comuns incluem:\n",
        "        *   **Hold-out interno:** Calcular as médias em um conjunto de treino e aplicá-las a um conjunto de validação/teste.\n",
        "        *   **Cross-Validation:** Dentro de cada fold de validação cruzada, calcular as médias apenas nos dados de treino do fold e aplicá-las aos dados de validação do fold.\n",
        "        *   **Smoothing (Suavização):** Combina a média da categoria com a média global do target, dando mais peso à média global para categorias com poucas amostras. Fórmula comum: `encoded = (mean_category * count + mean_global * weight) / (count + weight)`. `weight` é um hiperparâmetro.\n",
        "*   **Como utilizá-lo? (Exemplo conceitual com Smoothing)**\n",
        "    ```python\n",
        "    import pandas as pd\n",
        "    from sklearn.model_selection import KFold\n",
        "\n",
        "    # Dados de exemplo: Cidade, Target (e.g., Comprou=1, Não Comprou=0)\n",
        "    data = {'Cidade': ['SP', 'RJ', 'SP', 'BH', 'RJ', 'SP', 'BH', 'SP', 'RJ', 'BH'],\n",
        "            'Target': [  1,    0,    1,    0,    1,    0,    1,    1,    0,    0]}\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # --- Implementação CUIDADOSA com KFold e Smoothing ---\n",
        "    n_splits = 5 # Exemplo KFold\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    feature = 'Cidade'\n",
        "    target = 'Target'\n",
        "    weight = 10 # Hiperparâmetro de suavização\n",
        "\n",
        "    # Calcular a média global do target\n",
        "    global_mean = df[target].mean()\n",
        "\n",
        "    # Criar coluna para o encoding (inicializada com NaN ou média global)\n",
        "    df[f'{feature}_TargetEncoded'] = global_mean\n",
        "\n",
        "    # Iterar pelos folds da validação cruzada\n",
        "    for train_index, val_index in kf.split(df):\n",
        "        X_train, X_val = df.iloc[train_index], df.iloc[val_index]\n",
        "\n",
        "        # Calcular médias e contagens SOMENTE no fold de treino atual\n",
        "        means = X_train.groupby(feature)[target].mean()\n",
        "        counts = X_train.groupby(feature)[target].count()\n",
        "\n",
        "        # Aplicar smoothing\n",
        "        smooth_means = (means * counts + global_mean * weight) / (counts + weight)\n",
        "\n",
        "        # Mapear as médias suavizadas para o fold de VALIDAÇÃO atual\n",
        "        df.loc[val_index, f'{feature}_TargetEncoded'] = X_val[feature].map(smooth_means)\n",
        "\n",
        "        # Preencher NaNs que podem surgir (categorias raras só no fold de validação) com a média global\n",
        "        df[f'{feature}_TargetEncoded'].fillna(global_mean, inplace=True)\n",
        "\n",
        "\n",
        "    # Para aplicar em NOVOS dados (teste): Calcular médias/contagens em TODO o treino e aplicar.\n",
        "    # Exemplo simplificado para aplicar no dataset inteiro (pós CV para treino):\n",
        "    means_full_train = df.groupby(feature)[target].mean() # Idealmente do treino completo original\n",
        "    counts_full_train = df.groupby(feature)[target].count()\n",
        "    smooth_means_full = (means_full_train * counts_full_train + global_mean * weight) / (counts_full_train + weight)\n",
        "    # df['New_Data_Encoded'] = df['Cidade'].map(smooth_means_full).fillna(global_mean) # Para novos dados\n",
        "\n",
        "    print(\"Target Encoding com KFold Smoothing:\\n\", df)\n",
        "    ```\n",
        "    *Nota: Bibliotecas como `category_encoders` simplificam isso, mas entender o KFold/Smoothing é crucial.*\n",
        "*   **Quais suas vantagens e limitações?**\n",
        "    *   **Vantagens:** Pode ser muito preditivo, captura relação com o target. Não aumenta dimensionalidade como OHE. Ótimo para alta cardinalidade.\n",
        "    *   **Limitações:** **Alto risco de overfitting e target leakage** se não implementado corretamente (usando CV, smoothing). Sensível a categorias raras (smoothing ajuda). Menos interpretável que OHE. Principalmente para classificação/regressão.\n",
        "    *   **Precauções:** **Implementação cuidadosa é MANDATÓRIA.** Use KFold/Leave-One-Out CV para gerar os encodings no treino. Use smoothing.\n",
        "*   **Quais as principais dicas práticas?**\n",
        "    *   **\"Game changer\" em competições**, mas perigoso se mal feito.\n",
        "    *   Sempre use alguma forma de regularização (smoothing, CV). O `weight` no smoothing é um hiperparâmetro importante para tunar.\n",
        "    *   Pode vazar informação sobre a distribuição do target no teste se categorias raras no treino tiverem médias muito diferentes da média global.\n",
        "    *   Considere adicionar ruído gaussiano pequeno aos encodings como outra forma de regularização.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I0_SVyvP_dqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4. Weight of Evidence (WoE)**\n"
      ],
      "metadata": {
        "id": "Ds5Qak5PXrUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   **O que é?**\n",
        "    *   **Motivação:** Criar uma **transformação monotônica** entre a feature (geralmente categórica ou numérica binarizada) e o **log-odds** do **target binário**. Popular em **modelagem de risco de crédito** para interpretabilidade com Regressão Logística.\n",
        "    *   **Proposta:** Substituir cada categoria (ou bin de uma variável contínua) pelo seu valor de Weight of Evidence.\n",
        "*   **Qual o seu funcionamento?**\n",
        "    *   **Pressupostos:** Funciona melhor com target binário (event/non-event, good/bad). Variáveis contínuas precisam ser binarizadas (agrupadas) primeiro.\n",
        "    *   **Lógica:** Para cada categoria/bin 'i':\n",
        "        1.  Calcular a % de 'Bons' (evento=1) na categoria 'i': `%Bons_i = Count(Bons | Categoria=i) / Total Bons`\n",
        "        2.  Calcular a % de 'Maus' (evento=0) na categoria 'i': `%Maus_i = Count(Maus | Categoria=i) / Total Maus`\n",
        "        3.  Calcular WoE: `WoE_i = ln(%Bons_i / %Maus_i)`\n",
        "        *   WoE > 0: Categoria tem mais 'Bons' que a média.\n",
        "        *   WoE < 0: Categoria tem mais 'Maus' que a média.\n",
        "        *   WoE ~ 0: Categoria tem distribuição similar à média.\n",
        "    *   **Information Value (IV):** Frequentemente calculado junto com WoE para selecionar features. `IV = sum((%Bons_i - %Maus_i) * WoE_i)` por toda as categorias `i`. Mede o poder preditivo geral da feature.\n",
        "*   **Como utilizá-lo? (Exemplo conceitual)**\n",
        "    ```python\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    # Dados de exemplo: Risco (Categórico), Target (Inadimplente=1, Adimplente=0)\n",
        "    data = {'Risco': ['Alto', 'Baixo', 'Medio', 'Alto', 'Baixo', 'Medio', 'Alto', 'Baixo', 'Medio', 'Alto'],\n",
        "            'Target': [  1,      0,       0,      1,      0,       1,       0,      0,       1,      1]}\n",
        "    df = pd.DataFrame(data)\n",
        "    feature = 'Risco'\n",
        "    target = 'Target'\n",
        "\n",
        "    # Calcular totais de Bons (Target=0) e Maus (Target=1)\n",
        "    total_maus = df[target].sum()\n",
        "    total_bons = len(df) - total_maus\n",
        "\n",
        "    # Agrupar para calcular contagens por categoria\n",
        "    agg_df = df.groupby(feature)[target].agg(['count', 'sum']) # sum = count de Maus (Target=1)\n",
        "    agg_df.rename(columns={'count': 'Total_Obs', 'sum': 'Maus'}, inplace=True)\n",
        "    agg_df['Bons'] = agg_df['Total_Obs'] - agg_df['Maus']\n",
        "\n",
        "    # Calcular percentuais (adicionando pequeno valor para evitar divisão por zero)\n",
        "    epsilon = 1e-6\n",
        "    agg_df['%Bons'] = (agg_df['Bons'] + epsilon) / (total_bons + epsilon)\n",
        "    agg_df['%Maus'] = (agg_df['Maus'] + epsilon) / (total_maus + epsilon)\n",
        "\n",
        "    # Calcular WoE\n",
        "    agg_df['WoE'] = np.log(agg_df['%Bons'] / agg_df['%Maus'])\n",
        "\n",
        "    # Calcular contribuição para IV\n",
        "    agg_df['IV_contrib'] = (agg_df['%Bons'] - agg_df['%Maus']) * agg_df['WoE']\n",
        "    iv_total = agg_df['IV_contrib'].sum()\n",
        "\n",
        "    print(\"Weight of Evidence (WoE) e Information Value (IV):\\n\", agg_df)\n",
        "    print(f\"\\nInformation Value Total para '{feature}': {iv_total:.4f}\")\n",
        "\n",
        "    # Mapear WoE de volta ao dataframe original (usar apenas no treino/aplicar no teste)\n",
        "    woe_map = agg_df['WoE'].to_dict()\n",
        "    df[f'{feature}_WoE'] = df[feature].map(woe_map)\n",
        "    print(\"\\nDataFrame com WoE mapeado:\\n\", df)\n",
        "    ```\n",
        "*   **Quais suas vantagens e limitações?**\n",
        "    *   **Vantagens:** Cria relação monotônica com log-odds, **ótimo para interpretabilidade de modelos lineares** (especialmente Regressão Logística). Lida bem com categorias. **IV fornece métrica de seleção de features**.\n",
        "    *   **Limitações:** Principalmente para **target binário**. Requer binarização para features contínuas (a qualidade depende dos bins). **Sensível à estratégia de binarização**. Pode \"achatar\" informação dentro dos bins.\n",
        "    *   **Precauções:** Calcular WoE e IV **apenas** no conjunto de treino. **A binarização de contínuas é crítica e requer cuidado** (ex: usar Decision Tree para encontrar bons pontos de corte, garantir amostras suficientes por bin).\n",
        "*   **Quais as principais dicas práticas?**\n",
        "    *   **Padrão ouro em modelagem de risco de crédito.**\n",
        "    *   Use IV para seleção de features: < 0.02 (Inútil), 0.02-0.1 (Fraca), 0.1-0.3 (Média), 0.3-0.5 (Forte), > 0.5 (Suspeita/Muito Forte).\n",
        "    *   Garanta que cada bin/categoria tenha uma % mínima de observações (ex: 5%).\n",
        "    *   Verifique a monotonicidade do WoE após a binarização (geralmente desejável).\n"
      ],
      "metadata": {
        "id": "Z0mtWpasVgj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### III. Seleção de Features (Feature Selection)\n",
        "\n",
        "**Reduzir o número de features** de entrada para **melhorar o desempenho** do modelo, **reduzir overfitting**, **acelerar o treinamento** e **aumentar a interpretabilidade.**"
      ],
      "metadata": {
        "id": "LVXQbn8kE5vF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Métodos Filtro (Filter Methods)**\n"
      ],
      "metadata": {
        "id": "lDut1HHXGcv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **O que é?**\n",
        "    *   **Motivação:** **Remover features** irrelevantes ou redundantes rapidamente, **antes mesmo de treinar um modelo** complexo.\n",
        "    *   **Proposta:** Avaliar e classificar features com base em suas **características estatísticas intrínsecas** (relação com o target ou variância própria), **independentemente de qualquer modelo de ML**.\n",
        "*   **Qual o seu funcionamento?**\n",
        "    *   **Pressupostos:** Métricas estatísticas (**correlação, variância, testes de hipóteses**) são bons indicadores da utilidade da feature. Ignora **interações entre features** - no máximo, **bivariada** (comparando pares de features, como na correlação, **mas focando em redundância**, não em poder preditivo conjunto).\n",
        "    *   **Lógica:** Calcula uma métrica para cada feature (ou par) e filtra com base em um limiar. Exemplos:\n",
        "        *   **VarianceThreshold:** Remove features com **variância zero ou abaixo de um limiar** (features constantes ou quase constantes).\n",
        "        *   **Correlação:** Remove uma de um par de **features altamente correlacionadas** (redundância). Calcula a correlação de Pearson (num-num), Spearman (ordinal/não-linear).\n",
        "        *   **Testes Estatísticos (com o target):**\n",
        "            *   **ANOVA F-test:** Para features numéricas e target categórico. Mede a diferença de médias da feature entre as classes. (`f_classif`)\n",
        "            *   **Chi-Quadrado (χ²):** Para features categóricas e target categórico. Mede a dependência entre a feature e o target. (`chi2`)\n",
        "            *   **Informação Mútua:** Mede a dependência mútua entre duas variáveis (numéricas ou categóricas), capturando relações não lineares. (`mutual_info_classif`, `mutual_info_regression`)\n",
        "*   **Como utilizá-lo? (Python com `scikit-learn` e `pandas`)**\n",
        "   "
      ],
      "metadata": {
        "id": "r9i0Kur4E4E-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, chi2, mutual_info_classif\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "\n",
        "data = {\n",
        "    'N1': np.random.rand(100) * 10,\n",
        "    'N2': np.random.rand(100) * 5 + 2,\n",
        "    'N_QuaseConstante': [1]*98 + [1.1, 1.2],\n",
        "    'C1': np.random.choice(['A', 'B', 'C', 'D'], 100),\n",
        "    'C2': np.random.choice(['X', 'Y'], 100),\n",
        "    'Target_Class': np.random.randint(0, 3, 100), # Target Categórico (0, 1, 2)\n",
        "    'Target_Reg': np.random.rand(100) * 50 # Target Numérico\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "# Instancia selector variance\n",
        "selector_variance = VarianceThreshold(threshold=0.01) # Remove features com variância menor ou igual a 0.01\n",
        "\n",
        "# Especifica features numéricas\n",
        "df_num_features = df.select_dtypes(include=['number']).drop(['Target_Class','Target_Reg'], axis=1)\n",
        "\n",
        "# Aplicar selecto variance as features numéricas\n",
        "selector_variance.fit(df_num_features)\n",
        "\n",
        "# Identifica features que foram selecionadas\n",
        "print(f\"Features pré VarianceThreshold: {df_num_features.columns.values}\")\n",
        "print(f\"Features pós VarianceThreshold: {selector_variance.get_feature_names_out()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y--_GsuEIwHd",
        "outputId": "d7a7f973-4aee-478c-8728-848de21b8e43"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features pré VarianceThreshold: ['N1' 'N2' 'N_QuaseConstante']\n",
            "Features pós VarianceThreshold: ['N1' 'N2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, chi2, mutual_info_classif\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "\n",
        "# Dados de exemplo (Numéricas: N1, N2, N_QuaseConstante; Categóricas: C1, C2)\n",
        "data = {\n",
        "    'N1': np.random.rand(100) * 10,\n",
        "    'N2': np.random.rand(100) * 5 + 2,\n",
        "    'N_QuaseConstante': [1]*98 + [1.1, 1.2],\n",
        "    'C1': np.random.choice(['A', 'B', 'C', 'D'], 100),\n",
        "    'C2': np.random.choice(['X', 'Y'], 100),\n",
        "    'Target_Class': np.random.randint(0, 3, 100), # Target Categórico (0, 1, 2)\n",
        "    'Target_Reg': np.random.rand(100) * 50 # Target Numérico\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "display(df.head())\n",
        "display(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "yOUEWuJaMWIu",
        "outputId": "d57946b4-9544-4c9a-ce97-ed5a61d57a79"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         N1        N2  N_QuaseConstante C1 C2  Target_Class  Target_Reg\n",
              "0  1.139764  3.585621               1.0  D  X             1   22.520008\n",
              "1  5.071230  3.027857               1.0  A  X             0   25.439216\n",
              "2  4.652684  4.920211               1.0  B  X             1   30.206564\n",
              "3  9.264858  4.087303               1.0  B  Y             1   48.612184\n",
              "4  4.387268  2.724174               1.0  B  X             2    9.136977"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a9e02108-cb9a-45b3-8745-93b1236aac30\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>N1</th>\n",
              "      <th>N2</th>\n",
              "      <th>N_QuaseConstante</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>Target_Class</th>\n",
              "      <th>Target_Reg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.139764</td>\n",
              "      <td>3.585621</td>\n",
              "      <td>1.0</td>\n",
              "      <td>D</td>\n",
              "      <td>X</td>\n",
              "      <td>1</td>\n",
              "      <td>22.520008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.071230</td>\n",
              "      <td>3.027857</td>\n",
              "      <td>1.0</td>\n",
              "      <td>A</td>\n",
              "      <td>X</td>\n",
              "      <td>0</td>\n",
              "      <td>25.439216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.652684</td>\n",
              "      <td>4.920211</td>\n",
              "      <td>1.0</td>\n",
              "      <td>B</td>\n",
              "      <td>X</td>\n",
              "      <td>1</td>\n",
              "      <td>30.206564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9.264858</td>\n",
              "      <td>4.087303</td>\n",
              "      <td>1.0</td>\n",
              "      <td>B</td>\n",
              "      <td>Y</td>\n",
              "      <td>1</td>\n",
              "      <td>48.612184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.387268</td>\n",
              "      <td>2.724174</td>\n",
              "      <td>1.0</td>\n",
              "      <td>B</td>\n",
              "      <td>X</td>\n",
              "      <td>2</td>\n",
              "      <td>9.136977</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9e02108-cb9a-45b3-8745-93b1236aac30')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a9e02108-cb9a-45b3-8745-93b1236aac30 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a9e02108-cb9a-45b3-8745-93b1236aac30');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cec7016d-886c-4ebc-a388-0064987f6bab\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cec7016d-886c-4ebc-a388-0064987f6bab')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cec7016d-886c-4ebc-a388-0064987f6bab button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"N1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.895888927287505,\n        \"min\": 1.139764240275355,\n        \"max\": 9.264858323166038,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          5.071230480536459,\n          4.387267695260037,\n          4.65268394603605\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"N2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8733867205881921,\n        \"min\": 2.7241743197518185,\n        \"max\": 4.92021052922277,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3.027856718612034,\n          2.7241743197518185,\n          4.92021052922277\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"N_QuaseConstante\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"C1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"D\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"C2\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Y\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target_Class\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target_Reg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.307297797016265,\n        \"min\": 9.13697697971756,\n        \"max\": 48.61218384382957,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          25.439215990865982\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(100, 7)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Filtro de Variância ---\n",
        "selector_var = VarianceThreshold(threshold=0.01) # Remove features com variância <= 0.01\n",
        "\n",
        "# Aplicar em dados numéricos (idealmente após escalonamento se as escalas originais variarem muito)\n",
        "N_features = ['N1', 'N2', 'N_QuaseConstante']\n",
        "selector_var.fit(df[N_features])\n",
        "print(\"Features mantidas após VarianceThreshold:\",\n",
        "      selector_var.get_feature_names_out(N_features))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6f-RwtWM3S6",
        "outputId": "69251e64-351c-4af1-edf5-b64a18e18ccf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features mantidas após VarianceThreshold: ['N1' 'N2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Filtro de Correlação (Exemplo) ---\n",
        "correlation_matrix = df[['N1', 'N2']].corr()\n",
        "print(\"\\nMatriz de Correlação:\\n\", correlation_matrix)\n",
        "\n",
        "# Tipicamente, se |corr| > 0.8 ou 0.9, considera-se remover uma das features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcmCt1ltM5Si",
        "outputId": "c61e5461-d942-4365-e2fb-64954ec80a12"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Matriz de Correlação:\n",
            "         N1      N2\n",
            "N1  1.0000  0.1078\n",
            "N2  0.1078  1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Filtros baseados no Target (SelectKBest) ---\n",
        "\n",
        "# --- Cenário 1: Features Numéricas vs Target Categórico (ANOVA F-test) ---\n",
        "\n",
        "# 1. Escolher a métrica e o número de features (k)\n",
        "#    SelectKBest: Classe que seleciona features com base nos 'k' maiores scores.\n",
        "#    score_func=f_classif: Especifica a função de pontuação a ser usada.\n",
        "#        f_classif: Calcula o teste F de ANOVA (Análise de Variância).\n",
        "#                   Este teste é adequado para:\n",
        "#                       - Features de entrada (X): Numéricas\n",
        "#                       - Variável Alvo (y): Categórica\n",
        "#                   Ele mede se as médias da feature numérica são significativamente\n",
        "#                   diferentes entre os grupos definidos pelo target categórico.\n",
        "#                   Um valor F alto e um p-valor baixo sugerem que a feature ajuda\n",
        "#                   a discriminar as classes do target.\n",
        "#    k=1: Queremos selecionar apenas a UMA (k=1) melhor feature numérica\n",
        "#         com base no score F de ANOVA.\n",
        "selector_f_classif = SelectKBest(score_func=f_classif, k=1)\n",
        "\n",
        "# 2. Aplicar o seletor aos dados\n",
        "#    df[['N1', 'N2']]: Selecionamos apenas as features NUMÉRICAS de entrada.\n",
        "#    df['Target_Class']: Fornecemos a variável alvo CATEGÓRICA.\n",
        "#    .fit(X, y): Este método faz o seguinte:\n",
        "#        a. Para CADA feature em X ('N1' e 'N2'):\n",
        "#           Calcula o score usando a função especificada (f_classif) em relação a 'y'.\n",
        "#           Neste caso, calcula o F-score de ANOVA para 'N1' vs 'Target_Class'\n",
        "#           e para 'N2' vs 'Target_Class'.\n",
        "#        b. Armazena internamente os scores calculados para todas as features.\n",
        "#        c. Identifica qual(is) feature(s) tem(êm) os 'k' maiores scores.\n",
        "#    Importante: O método `fit` NÃO modifica os dados originais (X ou y).\n",
        "#                Ele apenas calcula e armazena os resultados da análise.\n",
        "selector_f_classif.fit(df[['N1', 'N2']], df['Target_Class'])\n",
        "\n",
        "# 3. Analisar os resultados\n",
        "#    .get_feature_names_out(['N1', 'N2']): Retorna os nomes das features que foram\n",
        "#                                          SELECIONADAS (as 'k' melhores).\n",
        "#                                          Passar os nomes originais ajuda a obter\n",
        "#                                          nomes significativos na saída.\n",
        "print(\"\\nMelhor feature numérica para Target_Class (ANOVA):\",\n",
        "      selector_f_classif.get_feature_names_out(['N1', 'N2']))\n",
        "\n",
        "#    .scores_: Atributo que armazena os scores calculados para TODAS as features\n",
        "#              que foram avaliadas durante o `fit`, na mesma ordem em que foram passadas.\n",
        "#              Permite ver a pontuação de cada feature, não apenas da(s) selecionada(s).\n",
        "print(\"Scores ANOVA:\", selector_f_classif.scores_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bHDuRN3NDya",
        "outputId": "62aecec7-872e-43a5-e237-179370dfdad1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Melhor feature numérica para Target_Class (ANOVA): ['N1']\n",
            "Scores ANOVA: [0.58832312 0.4686794 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Filtros baseados no Target (SelectKBest) ---\n",
        "\n",
        "# --- Cenário 2: Features Categóricas vs Target Categórico (Chi2) ---\n",
        "\n",
        "# 4. Pré-processamento NECESSÁRIO para Chi2\n",
        "#    O teste Qui-Quadrado (Chi2) opera sobre tabelas de contingência, comparando\n",
        "#    frequências observadas com frequências esperadas sob a hipótese de independência.\n",
        "#    Ele requer que as entradas sejam NUMÉRICAS e NÃO-NEGATIVAS.\n",
        "#    Nossas features 'C1' e 'C2' são strings ('A', 'B', 'X', 'Y', etc.).\n",
        "#    LabelEncoder: Converte cada categoria string em um número inteiro (0, 1, 2...).\n",
        "#                  É uma forma rápida de obter a representação numérica necessária.\n",
        "#                  NOTA: Embora introduza uma ordem artificial (que Chi2 não usa),\n",
        "#                        é comum usá-lo aqui para aplicar o teste em filtros.\n",
        "#                        OneHotEncoding também funcionaria, mas geraria mais colunas\n",
        "#                        e exigiria aplicar Chi2 a cada coluna binária resultante.\n",
        "le = LabelEncoder()\n",
        "df['C1_Encoded'] = le.fit_transform(df['C1'])\n",
        "df['C2_Encoded'] = le.fit_transform(df['C2'])\n",
        "\n",
        "# 5. Escolher a métrica e k para features categóricas\n",
        "#    SelectKBest: Mesma classe de antes.\n",
        "#    score_func=chi2: Especifica a função de pontuação Qui-Quadrado.\n",
        "#        chi2: Adequado para:\n",
        "#                - Features de entrada (X): Categóricas (codificadas como numéricas não-negativas)\n",
        "#                - Variável Alvo (y): Categórica\n",
        "#              Testa a hipótese nula de que a feature e o target são independentes.\n",
        "#              Um valor Chi2 alto e um p-valor baixo sugerem uma dependência\n",
        "#              entre a feature e o target (ou seja, a feature é relevante).\n",
        "#    k=1: Queremos selecionar a UMA melhor feature categórica (codificada).\n",
        "selector_chi2 = SelectKBest(score_func=chi2, k=1)\n",
        "\n",
        "# 6. Aplicar o seletor aos dados CATEGÓRICOS CODIFICADOS\n",
        "#    df[['C1_Encoded', 'C2_Encoded']]: Usamos as versões NUMÉRICAS das features categóricas.\n",
        "#    df['Target_Class']: O target ainda é o mesmo (categórico).\n",
        "#    .fit(X, y): Calcula o score Chi2 para 'C1_Encoded' vs 'Target_Class'\n",
        "#                e para 'C2_Encoded' vs 'Target_Class'. Armazena os scores\n",
        "#                e identifica a feature com o maior score (top k=1).\n",
        "selector_chi2.fit(df[['C1_Encoded', 'C2_Encoded']], df['Target_Class'])\n",
        "\n",
        "# 7. Analisar os resultados para Chi2\n",
        "#    .get_feature_names_out(['C1_Encoded', 'C2_Encoded']): Mostra qual feature codificada foi selecionada.\n",
        "print(\"\\nMelhor feature categórica para Target_Class (Chi2):\",\n",
        "       selector_chi2.get_feature_names_out(['C1_Encoded', 'C2_Encoded']))\n",
        "#    .scores_: Mostra os scores Chi2 para ambas as features codificadas.\n",
        "print(\"Scores Chi2:\", selector_chi2.scores_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDPFTm_6IhiW",
        "outputId": "8585dfd1-6bca-40c3-d15a-281f7b8075fd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Melhor feature categórica para Target_Class (Chi2): ['C1_Encoded']\n",
            "Scores Chi2: [0.64203457 0.45190814]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cenário 3: Informação Mútua (Exemplo de uso direto da função de score) ---\n",
        "\n",
        "# 8. Calcular Scores de Informação Mútua\n",
        "#    mutual_info_classif: Calcula a Informação Mútua entre cada feature e o target discreto.\n",
        "#        Informação Mútua (MI): Mede a redução na incerteza sobre o target 'y'\n",
        "#                                ao conhecer o valor da feature 'X'.\n",
        "#                                Vantagens:\n",
        "#                                   - Captura dependências NÃO LINEARES.\n",
        "#                                   - Pode ser usada para features numéricas ou categóricas.\n",
        "#                                   - Valor >= 0. Zero significa independência.\n",
        "#    df[['N1', 'N2', 'C1_Encoded', 'C2_Encoded']]: Passamos todas as features (numéricas e categóricas codificadas).\n",
        "#    df['Target_Class']: O target categórico.\n",
        "#    discrete_features=[False, False, True, True]: Um *hint* importante para a função.\n",
        "#        Indica quais colunas em X devem ser tratadas como discretas/categóricas.\n",
        "#        'N1', 'N2' são contínuas (False).\n",
        "#        'C1_Encoded', 'C2_Encoded' são discretas (True).\n",
        "#        Isso ajuda o algoritmo a usar os estimadores corretos de MI.\n",
        "#    A função retorna diretamente um array com os scores MI para cada feature.\n",
        "mi_scores = mutual_info_classif(df[['N1', 'N2', 'C1_Encoded', 'C2_Encoded']], df['Target_Class'], discrete_features=[False, False, True, True])\n",
        "\n",
        "# 9. Exibir os scores MI\n",
        "#    Colocamos os scores em uma Série Pandas para melhor visualização com os nomes das features.\n",
        "print(\"\\nScores de Informação Mútua (vs Target_Class):\\n\", pd.Series(mi_scores, index=['N1', 'N2', 'C1_Encoded', 'C2_Encoded']))\n",
        "#    NOTA: Aqui, apenas calculamos os scores. Se quiséssemos usar MI dentro do\n",
        "#          SelectKBest, faríamos: SelectKBest(score_func=mutual_info_classif, k=...)\n",
        "#          e depois o `.fit()`. Usar a função diretamente é útil para apenas\n",
        "#          ver os scores e decidir um limiar manualmente, ou para usar em cenários\n",
        "#          onde os hints como `discrete_features` são necessários e SelectKBest\n",
        "#          pode não passá-los diretamente (embora versões recentes possam ter melhorado isso)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWENB8fXXTA0",
        "outputId": "5b7bc336-d8fc-4225-d634-e2b9bfea4cf0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scores de Informação Mútua (vs Target_Class):\n",
            " N1            0.000000\n",
            "N2            0.000000\n",
            "C1_Encoded    0.007296\n",
            "C2_Encoded    0.003625\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Quais suas vantagens e limitações?**\n",
        "    *   **Vantagens:** Rápido, computacionalmente barato. **Independente do modelo** (resultados podem ser usados para qualquer algoritmo). **Bom para uma primeira triagem** e redução de dimensionalidade.\n",
        "    *   **Limitações:** **Ignora interações entre features** (uma feature pode ser ruim sozinha, mas ótima em combinação com outra). A escolha do limiar/método é crucial e pode não ser ótima para o modelo final. **Não garante o melhor subconjunto de features para um *modelo específico***.\n",
        "    *   **Precauções:** Escolha a métrica estatística apropriada para o tipo de feature e target (num/cat). Cuidado com a interpretação de correlação (não implica causalidade).\n",
        "*   **Quais as principais dicas práticas?**\n",
        "    *   **Sempre comece removendo features de variância zero ou muito baixa.**\n",
        "    *   Verifique altas correlações (>0.9) entre features e **remova as redundantes** (**mantenha** a que tiver **maior correlação com o target** ou a mais interpretável).\n",
        "    *   Use testes estatísticos (ANOVA, Chi2, MI) para classificar features com base na relação com o target, **mas não confie *apenas* nisso**. Use como guia.\n",
        "    *   Combine com conhecimento de domínio para validar se a remoção faz sentido.\n",
        "\n"
      ],
      "metadata": {
        "id": "mpyccP_YIV7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Métodos Empacotamento (Wrapper Methods)**"
      ],
      "metadata": {
        "id": "HfkRqCDQZP3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   **O que é?**\n",
        "    *   **Motivação:** Selecionar features considerando diretamente o **desempenho de um modelo de ML específico**. Supera a limitação dos filtros de ignorar interações.\n",
        "    *   **Proposta:** Tratar a seleção de features como um **problema de busca**, onde diferentes **subconjuntos de features são usados para treinar e avaliar um modelo**. O subconjunto que resulta no melhor desempenho (segundo uma métrica escolhida) é selecionado.\n",
        "*   **Qual o seu funcionamento?**\n",
        "    *   **Pressupostos:** O desempenho do modelo escolhido no wrapper é um bom proxy para o desempenho final. O espaço de busca pode ser explorado eficientemente.\n",
        "    *   **Lógica:** Utiliza um algoritmo de ML como \"caixa preta\". Exemplos de estratégias de busca:\n",
        "        *   **Forward Selection:** Começa sem features, adiciona uma por vez (a que mais melhora o desempenho) até um critério de parada.\n",
        "        *   **Backward Elimination:** Começa com todas as features, remove uma por vez (a que menos prejudica o desempenho, ou cuja remoção mais melhora) até um critério de parada.\n",
        "        *   **Recursive Feature Elimination (RFE):** Treina o modelo, remove a feature menos importante (baseado em coeficientes ou feature importance), retreina com as restantes, repetindo até o número desejado de features.\n",
        "        *   **Sequential Feature Selector (SFS):** Implementação moderna no Scikit-learn para Forward/Backward selection, com suporte a CV.\n",
        "*   **Como utilizá-lo? (Python com `scikit-learn`)**\n",
        "  \n"
      ],
      "metadata": {
        "id": "k6OwTzaMZNRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE, RFECV, SequentialFeatureSelector\n",
        "from sklearn.linear_model import LogisticRegression # Exemplo de estimador\n",
        "from sklearn.ensemble import RandomForestClassifier # Outro exemplo\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "metadata": {
        "id": "Lk9F1LunykyT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dados de exemplo (Numéricas: N1, N2, N_QuaseConstante; Categóricas: C1, C2)\n",
        "data = {\n",
        "    'N1': np.random.rand(100) * 10,\n",
        "    'N2': np.random.rand(100) * 5 + 2,\n",
        "    'N_QuaseConstante': [1]*98 + [1.1, 1.2],\n",
        "    'C1': np.random.choice(['A', 'B', 'C', 'D'], 100),\n",
        "    'C2': np.random.choice(['X', 'Y'], 100),\n",
        "    'Target_Class': np.random.randint(0, 3, 100), # Target Categórico (0, 1, 2)\n",
        "    'Target_Reg': np.random.rand(100) * 50 # Target Numérico\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "display(df.head())\n",
        "display(df.shape)"
      ],
      "metadata": {
        "id": "0lm8_xBpHjw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (usando os dados numéricos e target categórico do exemplo anterior) ...\n",
        "X = df[['N1', 'N2']] # Features numéricas\n",
        "y = df['Target_Class']"
      ],
      "metadata": {
        "id": "YsnM5PR1yzPI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- RFE (Recursive Feature Elimination) ---\n",
        "\n",
        "# Escolher um estimador (modelo) que forneça pesos/importâncias\n",
        "estimator_lr = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# Selecionar N features (ex: 1)\n",
        "selector_rfe = RFE(estimator=estimator_lr, n_features_to_select=1, step=1) # step=1 remove 1 por vez\n",
        "selector_rfe.fit(X, y)\n",
        "print(\"\\nRFE Seleção (n=1):\", selector_rfe.support_) # True/False para cada feature\n",
        "print(\"RFE Ranking:\", selector_rfe.ranking_) # 1 = selecionada, 2 = eliminada primeiro, etc.\n",
        "print(\"Features selecionadas por RFE:\", X.columns[selector_rfe.support_])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RdtFAQ4y04T",
        "outputId": "0e3ae168-6730-4e49-f3f2-9f9a5ff84ca9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RFE Seleção (n=1): [False  True]\n",
            "RFE Ranking: [2 1]\n",
            "Features selecionadas por RFE: Index(['N2'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- RFECV (RFE com Cross-Validation para encontrar N ótimo) ---\n",
        "\n",
        "# Usa CV para determinar o número ideal de features\n",
        "cv_strategy = StratifiedKFold(5)\n",
        "estimator_rf = RandomForestClassifier(random_state=42, n_estimators=10) # Exemplo com RandomForest\n",
        "selector_rfecv = RFECV(estimator=estimator_rf, step=1, cv=cv_strategy, scoring='accuracy', min_features_to_select=1)\n",
        "selector_rfecv.fit(X, y) # Usa mais recursos computacionais\n",
        "print(f\"\\nRFECV: Número ótimo de features encontrado: {selector_rfecv.n_features_}\")\n",
        "print(\"RFECV Seleção:\", selector_rfecv.support_)\n",
        "print(\"Features selecionadas por RFECV:\", X.columns[selector_rfecv.support_])\n",
        "print(\"RFECV Scores por número de features:\", selector_rfecv.cv_results_['mean_test_score']) # Desempenho CV"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_L0Vbnd2hu6",
        "outputId": "f2beedcd-a8be-4955-f9f9-b4409e818b73"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RFECV: Número ótimo de features encontrado: 2\n",
            "RFECV Seleção: [ True  True]\n",
            "Features selecionadas por RFECV: Index(['N1', 'N2'], dtype='object')\n",
            "RFECV Scores por número de features: [0.25 0.31]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Sequential Feature Selector (SFS) ---\n",
        "\n",
        "# Forward Selection\n",
        "sfs_forward = SequentialFeatureSelector(estimator_lr, n_features_to_select='auto', # ou um número\n",
        "                                        tol=None, # Para n_features_to_select='auto'\n",
        "                                        direction='forward', scoring='accuracy', cv=cv_strategy, n_jobs=-1)\n",
        "sfs_forward.fit(X, y)\n",
        "print(\"\\nSFS Forward Seleção:\", sfs_forward.get_support())\n",
        "print(\"Features selecionadas por SFS Forward:\", sfs_forward.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tz6Qytb3Dtb",
        "outputId": "41e3993c-8c04-48d8-fb10-a800b5052161"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SFS Forward Seleção: [ True False]\n",
            "Features selecionadas por SFS Forward: ['N1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Sequential Feature Selector (SFS) ---\n",
        "\n",
        "# Backward Elimination (começa com todas)\n",
        "sfs_backward = SequentialFeatureSelector(estimator_lr, n_features_to_select=1, # Ex: selecionar 1\n",
        "                                          direction='backward', scoring='accuracy', cv=cv_strategy, n_jobs=-1)\n",
        "sfs_backward.fit(X, y)\n",
        "print(\"\\nSFS Backward Seleção (n=1):\", sfs_backward.get_support())\n",
        "print(\"Features selecionadas por SFS Backward:\", sfs_backward.get_feature_names_out())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVlg5d1ByA18",
        "outputId": "fee63686-d895-4448-eef4-15cd41adbc1a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SFS Backward Seleção (n=1): [ True False]\n",
            "Features selecionadas por SFS Backward: ['N1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Quais suas vantagens e limitações?**\n",
        "    *   **Vantagens:** Considera interações entre features (via desempenho do modelo). Pode encontrar **subconjuntos de features mais performáticos** do que **filtros**. **Adaptado ao modelo que será usado**.\n",
        "    *   **Limitações:** **Computacionalmente muito caro**, pois requer múltiplos treinamentos do modelo base. **Risco de overfitting no processo de seleção se não usar validação cruzada robusta** (como no RFECV/SFS com CV). **O resultado é específico para o modelo usado no wrapper**.\n",
        "    *   **Precauções:** Use validação cruzada (RFECV, SFS com `cv`) para evitar overfitting na seleção. Escolha um modelo base representativo ou o modelo final desejado (se o custo permitir). **Pode ser inviável para datasets muito grandes ou modelos muito lentos.**\n",
        "*   **Quais as principais dicas práticas?**\n",
        "    *   **RFECV é uma escolha popular e robusta** se o custo computacional for aceitável, **pois encontra o número ideal de features.**\n",
        "    *   Se o **custo for proibitivo**, use **RFE com um número fixo de features (escolhido por heurística ou filtro)** ou use um **modelo base mais rápido** (ex: Regressão Logística) no wrapper, mesmo que o modelo final seja outro (pode dar uma boa aproximação).\n",
        "    *   Para datasets muito largos (muitas features), comece com um filtro para reduzir o espaço de busca antes de aplicar um wrapper.\n"
      ],
      "metadata": {
        "id": "jjd1s_tux-MW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. Métodos Incorporados (Embedded Methods)**\n"
      ],
      "metadata": {
        "id": "7cNAWxjhZY9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   **O que é?**\n",
        "    *   **Motivação:** Realizar a seleção de features como **parte intrínseca do processo de treinamento do modelo**, buscando um equilíbrio entre eficiência computacional e desempenho.\n",
        "    *   **Proposta:** Utilizar **algoritmos de ML que possuem mecanismos internos de seleção de features** ou que atribuem **pesos/importâncias às features** durante o treinamento.\n",
        "*   **Qual o seu funcionamento?**\n",
        "    *   **Pressupostos:** O processo de otimização do modelo consegue identificar e penalizar/ignorar features menos relevantes.\n",
        "    *   **Lógica:** A seleção ocorre naturalmente durante o `fit`. Exemplos:\n",
        "        *   **Regularização L1 (Lasso):** Adiciona um termo de penalidade à função de custo igual à soma dos valores absolutos dos coeficientes (`alpha * sum(|coefs|)`). **Isso força coeficientes de features menos importantes a irem exatamente para zero**, efetivamente selecionando features.\n",
        "        *   **Modelos baseados em Árvores (Random Forest, Gradient Boosting):** Calculam a **importância de cada feature** com base em quanto ela contribui para a **redução da impureza** (Gini, entropia) ou erro nos nós das árvores. Features com baixa importância podem ser descartadas.\n",
        "        *   **Elastic Net:** Combina regularização L1 e L2.\n",
        "*   **Como utilizá-lo? (Python com `scikit-learn`)**\n",
        "    "
      ],
      "metadata": {
        "id": "WdwcwtfvZXLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LassoCV, Lasso\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "IsNYI2LcHWyr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Dados de exemplo (Numéricas: N1, N2, N_QuaseConstante; Categóricas: C1, C2)\n",
        "data = {\n",
        "    'N1': np.random.rand(100) * 10,\n",
        "    'N2': np.random.rand(100) * 5 + 2,\n",
        "    'N_QuaseConstante': [1]*98 + [1.1, 1.2],\n",
        "    'C1': np.random.choice(['A', 'B', 'C', 'D'], 100),\n",
        "    'C2': np.random.choice(['X', 'Y'], 100),\n",
        "    'Target_Class': np.random.randint(0, 3, 100), # Target Categórico (0, 1, 2)\n",
        "    'Target_Reg': np.random.rand(100) * 50 # Target Numérico\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "display(df.head())\n",
        "display(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "EivmcJjKHmNJ",
        "outputId": "a53326e2-d099-4b3a-d125-536daa43d188"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         N1        N2  N_QuaseConstante C1 C2  Target_Class  Target_Reg\n",
              "0  7.383292  5.037207               1.0  D  Y             1   20.383438\n",
              "1  7.085384  4.931399               1.0  C  X             1   32.472345\n",
              "2  9.541262  5.500711               1.0  A  Y             2    6.171159\n",
              "3  2.870205  3.544928               1.0  D  X             0    7.716680\n",
              "4  3.018764  6.579619               1.0  D  Y             0   11.698317"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-928699db-556c-4a50-8245-41c59ea2437f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>N1</th>\n",
              "      <th>N2</th>\n",
              "      <th>N_QuaseConstante</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>Target_Class</th>\n",
              "      <th>Target_Reg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.383292</td>\n",
              "      <td>5.037207</td>\n",
              "      <td>1.0</td>\n",
              "      <td>D</td>\n",
              "      <td>Y</td>\n",
              "      <td>1</td>\n",
              "      <td>20.383438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.085384</td>\n",
              "      <td>4.931399</td>\n",
              "      <td>1.0</td>\n",
              "      <td>C</td>\n",
              "      <td>X</td>\n",
              "      <td>1</td>\n",
              "      <td>32.472345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.541262</td>\n",
              "      <td>5.500711</td>\n",
              "      <td>1.0</td>\n",
              "      <td>A</td>\n",
              "      <td>Y</td>\n",
              "      <td>2</td>\n",
              "      <td>6.171159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.870205</td>\n",
              "      <td>3.544928</td>\n",
              "      <td>1.0</td>\n",
              "      <td>D</td>\n",
              "      <td>X</td>\n",
              "      <td>0</td>\n",
              "      <td>7.716680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.018764</td>\n",
              "      <td>6.579619</td>\n",
              "      <td>1.0</td>\n",
              "      <td>D</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>11.698317</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-928699db-556c-4a50-8245-41c59ea2437f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-928699db-556c-4a50-8245-41c59ea2437f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-928699db-556c-4a50-8245-41c59ea2437f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-66ee7c7d-b091-47f5-a398-312c43188546\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-66ee7c7d-b091-47f5-a398-312c43188546')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-66ee7c7d-b091-47f5-a398-312c43188546 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"N1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.9288836516068883,\n        \"min\": 2.8702045645637986,\n        \"max\": 9.541262435910335,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          7.0853835474095845,\n          3.018763884046175,\n          9.541262435910335\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"N2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0952964662672118,\n        \"min\": 3.544927747393846,\n        \"max\": 6.579618701248725,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4.931399249448052,\n          6.579618701248725,\n          5.500710781549498\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"N_QuaseConstante\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"C1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"D\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"C2\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"X\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target_Class\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target_Reg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.883372107991956,\n        \"min\": 6.171159444722024,\n        \"max\": 32.47234544538533,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          32.47234544538533\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(100, 7)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (usando os dados numéricos e target categórico do exemplo anterior) ...\n",
        "X = df[['N1', 'N2']] # Features numéricas\n",
        "y = df['Target_Class']"
      ],
      "metadata": {
        "id": "CeyiaG9-J-lH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Lasso (Regularização L1) ---\n",
        "\n",
        "# É importante escalar os dados para Lasso\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# LassoCV encontra o melhor alpha (força da regularização) via CV\n",
        "lasso_cv = LassoCV(cv=5, random_state=42, n_jobs=-1).fit(X_scaled, y)\n",
        "print(f\"\\nLassoCV: Melhor alpha encontrado: {lasso_cv.alpha_:.4f}\")\n",
        "\n",
        "# Treinar Lasso com o melhor alpha\n",
        "lasso = Lasso(alpha=lasso_cv.alpha_).fit(X_scaled, y)\n",
        "print(\"Coeficientes Lasso:\", lasso.coef_)\n",
        "\n",
        "# Seleciona features com coef. diferentes de zero\n",
        "selected_lasso_features = X.columns[lasso.coef_ != 0]\n",
        "print(\"Features selecionadas por Lasso (coef != 0):\", selected_lasso_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6dHGEb-H2WP",
        "outputId": "44820a52-3a70-48ed-b147-dd76957bfd23"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LassoCV: Melhor alpha encontrado: 0.0707\n",
            "Coeficientes Lasso: [ 0.00000000e+00 -2.66453526e-17]\n",
            "Features selecionadas por Lasso (coef != 0): Index(['N2'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Feature Importance de Árvores ---\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42, n_estimators=50)\n",
        "\n",
        "rf.fit(X, y) # Escalonamento menos crítico para RF puro, mas bom para interpretabilidade da importância\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "print(\"\\nImportância das Features (Random Forest):\", importances)\n",
        "\n",
        "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "display(feature_importance_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "f42Y_8m5MtgY",
        "outputId": "35d2a2c0-eb1b-4b9a-d259-a21c561ce3cf"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Importância das Features (Random Forest): [0.51425675 0.48574325]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Feature  Importance\n",
              "0      N1    0.514257\n",
              "1      N2    0.485743"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-29ac342e-c4be-437a-a145-f74da606ad12\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature</th>\n",
              "      <th>Importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>N1</td>\n",
              "      <td>0.514257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>N2</td>\n",
              "      <td>0.485743</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29ac342e-c4be-437a-a145-f74da606ad12')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-29ac342e-c4be-437a-a145-f74da606ad12 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-29ac342e-c4be-437a-a145-f74da606ad12');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bbe6c81a-ff55-4df9-8411-6aa5f43b7096\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bbe6c81a-ff55-4df9-8411-6aa5f43b7096')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bbe6c81a-ff55-4df9-8411-6aa5f43b7096 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_90f64a7f-6c81-411c-aa09-3f1df73ff932\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('feature_importance_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_90f64a7f-6c81-411c-aa09-3f1df73ff932 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('feature_importance_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "feature_importance_df",
              "summary": "{\n  \"name\": \"feature_importance_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Feature\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"N2\",\n          \"N1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Importance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02016208754069268,\n        \"min\": 0.48574325117709943,\n        \"max\": 0.5142567488229006,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.48574325117709943,\n          0.5142567488229006\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Usando SelectFromModel (Meta-transformador) ---\n",
        "\n",
        "# Seleciona features com base em um limiar de importância/coeficiente\n",
        "\n",
        "# Exemplo com RandomForest\n",
        "sfm_rf = SelectFromModel(estimator=RandomForestClassifier(random_state=42, n_estimators=50),\n",
        "                          threshold='median') # threshold pode ser numérico ('0.1'), ou 'mean', 'median'\n",
        "sfm_rf.fit(X, y)\n",
        "print(\"\\nSelectFromModel (RF, threshold='median') Seleção:\", sfm_rf.get_support())\n",
        "print(\"Features selecionadas por SFM (RF):\", sfm_rf.get_feature_names_out())\n",
        "\n",
        "# Exemplo com Lasso (requer dados escalados)\n",
        "sfm_lasso = SelectFromModel(estimator=Lasso(alpha=lasso_cv.alpha_))\n",
        "# threshold='auto' não funciona bem com Lasso (usa 1e-5), melhor usar coef != 0 manualmente ou especificar limiar\n",
        "\n",
        "sfm_lasso.fit(X_scaled, y) # Fit no escalado\n",
        "\n",
        "print(\"\\nSelectFromModel (Lasso) Seleção:\", sfm_lasso.get_support())\n",
        "# Get names do original X\n",
        "print(\"Features selecionadas por SFM (Lasso):\", X.columns[sfm_lasso.get_support()])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzO1V6EKHR-S",
        "outputId": "a24103be-3ed7-4852-e021-761f23c7a37e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SelectFromModel (RF, threshold='median') Seleção: [ True False]\n",
            "Features selecionadas por SFM (RF): ['N1']\n",
            "\n",
            "SelectFromModel (Lasso) Seleção: [False False]\n",
            "Features selecionadas por SFM (Lasso): Index([], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Quais suas vantagens e limitações?**\n",
        "    *   **Vantagens:** **Mais eficiente que wrappers** (seleção integrada ao treino). Considera interações (implícito no modelo). **Geralmente bom desempenho.**\n",
        "    *   **Limitações:** A seleção é **específica do modelo usado**. *Importâncias de árvores* podem ser **enviesadas para features numéricas ou categóricas de alta cardinalidade**. Lasso tende a selecionar aleatoriamente uma entre features altamente correlacionadas.\n",
        "    *   **Precauções:** **Escalone os dados para modelos lineares com regularização** (Lasso, Ridge, ElasticNet). Esteja ciente dos vieses da feature importance (considere **Permutation Importance** como alternativa mais robusta, embora mais custosa).\n",
        "*   **Quais as principais dicas práticas?**\n",
        "    *   **Lasso é excelente para criar modelos esparsos (muitos coeficientes zero)**, útil quando se suspeita que muitas features são irrelevantes.\n",
        "    *   **Feature importance de árvores é muito popular**, mas use com cautela. Verifique a estabilidade (ex: treine vários RF com seeds diferentes) e considere `PermutationImportance`.\n",
        "    *   `SelectFromModel` é uma forma conveniente de aplicar a seleção após treinar um modelo embedded.\n",
        "    *   Combine diferentes métodos: **Use filtros para uma redução inicial**, depois um método embedded ou wrapper para refinar a seleção."
      ],
      "metadata": {
        "id": "hG32QKRnHMkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IV. Redução de Dimensionalidade\n",
        "\n",
        "Técnicas que **transformam** os dados de um **espaço de alta dimensão** para um de **baixa dimensão**, criando **novas features (combinações das originais)** em vez de apenas *selecionar* features existentes."
      ],
      "metadata": {
        "id": "rL3DayLiEnm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. PCA (Principal Component Analysis - Análise de Componentes Principais)**\n"
      ],
      "metadata": {
        "id": "tPNLbRRFwKfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **O que é?**\n",
        "    *   **Motivação:** Reduzir a dimensionalidade **preservando o máximo possível da variância original dos dados**. Útil para **visualização**, compressão de dados, **remoção de multicolinearidade** e, às vezes, melhoria de desempenho de modelos sensíveis à dimensionalidade. É uma técnica **não supervisionada**.\n",
        "    *   **Proposta:** Encontrar um novo conjunto de **eixos ortogonais** (Componentes Principais - PCs) no **espaço das features originais**, tais que a **variância** dos dados projetados nesses eixos seja **maximizada**. Os PCs são ordenados pela quantidade de variância que explicam.\n",
        "*   **Qual o seu funcionamento?**\n",
        "    *   **Pressupostos:** Assume que as direções de maior variância nos dados são as mais importantes (contêm mais informação/sinal). *Funciona melhor com relações aproximadamente lineares*. **Requer que os dados estejam escalonados** (geralmente Padronizados com StandardScaler).\n",
        "    *   **Lógica:**\n",
        "        1.  Calcula a matriz de covariância dos dados (escalonados).\n",
        "        2.  Calcula os autovetores e autovalores dessa matriz.\n",
        "        3.  Os autovetores são os Componentes Principais (direções).\n",
        "        4.  Os autovalores indicam a variância explicada por cada PC correspondente.\n",
        "        5.  Ordena os PCs em ordem decrescente de autovalores.\n",
        "        6.  Seleciona os `k` primeiros PCs (onde `k` < número original de features).\n",
        "        7.  Projeta os dados originais nesses `k` PCs (produto escalar) para obter as novas features de dimensão reduzida.\n",
        "*   **Como utilizá-lo? (Python com `scikit-learn`)**\n",
        "   "
      ],
      "metadata": {
        "id": "2xOC2HyL1I-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Dados de exemplo (Numéricas: N1, N2, N_QuaseConstante; Categóricas: C1, C2)\n",
        "data = {\n",
        "    'N1': np.random.rand(100) * 10,\n",
        "    'N2': np.random.rand(100) * 5 + 2,\n",
        "    'N_QuaseConstante': [1]*98 + [1.1, 1.2],\n",
        "    'C1': np.random.choice(['A', 'B', 'C', 'D'], 100),\n",
        "    'C2': np.random.choice(['X', 'Y'], 100),\n",
        "    'Target_Class': np.random.randint(0, 3, 100), # Target Categórico (0, 1, 2)\n",
        "    'Target_Reg': np.random.rand(100) * 50 # Target Numérico\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "display(df.head())\n",
        "display(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "QMOXk0A41diT",
        "outputId": "9e5bd6e7-00ef-4f53-ec17-cbaf2b071213"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         N1        N2  N_QuaseConstante C1 C2  Target_Class  Target_Reg\n",
              "0  6.785536  3.282043               1.0  A  X             0   29.865205\n",
              "1  5.798454  3.005446               1.0  D  X             2   23.589024\n",
              "2  0.935057  3.847908               1.0  B  X             1   25.708349\n",
              "3  7.406977  2.688367               1.0  C  X             0   13.115794\n",
              "4  6.668963  5.801082               1.0  A  Y             0    0.168577"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-19310a94-f4f4-4a60-af51-5ce942da2dd8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>N1</th>\n",
              "      <th>N2</th>\n",
              "      <th>N_QuaseConstante</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>Target_Class</th>\n",
              "      <th>Target_Reg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.785536</td>\n",
              "      <td>3.282043</td>\n",
              "      <td>1.0</td>\n",
              "      <td>A</td>\n",
              "      <td>X</td>\n",
              "      <td>0</td>\n",
              "      <td>29.865205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.798454</td>\n",
              "      <td>3.005446</td>\n",
              "      <td>1.0</td>\n",
              "      <td>D</td>\n",
              "      <td>X</td>\n",
              "      <td>2</td>\n",
              "      <td>23.589024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.935057</td>\n",
              "      <td>3.847908</td>\n",
              "      <td>1.0</td>\n",
              "      <td>B</td>\n",
              "      <td>X</td>\n",
              "      <td>1</td>\n",
              "      <td>25.708349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.406977</td>\n",
              "      <td>2.688367</td>\n",
              "      <td>1.0</td>\n",
              "      <td>C</td>\n",
              "      <td>X</td>\n",
              "      <td>0</td>\n",
              "      <td>13.115794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.668963</td>\n",
              "      <td>5.801082</td>\n",
              "      <td>1.0</td>\n",
              "      <td>A</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>0.168577</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19310a94-f4f4-4a60-af51-5ce942da2dd8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-19310a94-f4f4-4a60-af51-5ce942da2dd8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-19310a94-f4f4-4a60-af51-5ce942da2dd8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b0263f42-f3de-4c7c-aac6-34c391493c0e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b0263f42-f3de-4c7c-aac6-34c391493c0e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b0263f42-f3de-4c7c-aac6-34c391493c0e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"N1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.625910248369789,\n        \"min\": 0.9350565504413555,\n        \"max\": 7.406977274852451,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          5.7984542985139145,\n          6.668962863499354,\n          0.9350565504413555\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"N2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2363048654686328,\n        \"min\": 2.6883666528821593,\n        \"max\": 5.801081679982009,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3.005445971463179,\n          5.801081679982009,\n          3.847908140547082\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"N_QuaseConstante\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"C1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"D\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"C2\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Y\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target_Class\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target_Reg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11.958851338989732,\n        \"min\": 0.16857743772809575,\n        \"max\": 29.865205047643585,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          23.589023762052737\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(100, 7)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (usando os dados numéricos e target categórico do exemplo anterior) ...\n",
        "X = df[['N1', 'N2']] # Features numéricas\n",
        "y = df['Target_Class']"
      ],
      "metadata": {
        "id": "1Jq9Jwh61eh6"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ... (usando X e y do exemplo anterior, features N1, N2) ...\n",
        "X = df[['N1', 'N2']]\n",
        "\n",
        "# 1. Escalonar os dados (CRUCIAL para PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 2. Instanciar PCA\n",
        "# Especificar n_components:\n",
        "# - int: número de componentes a manter (e.g., n_components=1)\n",
        "# - float (0 a 1): % de variância a ser explicada (e.g., n_components=0.95)\n",
        "# - None: mantém todos os componentes (útil para ver variância explicada)\n",
        "pca = PCA(n_components=None) # Primeiro, ver variância explicada\n",
        "\n",
        "# 3. Ajustar PCA SOMENTE nos dados de treino (aqui, X_scaled)\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# 4. Analisar variância explicada\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
        "print(\"\\nVariância Explicada por Componente:\", explained_variance_ratio)\n",
        "print(\"Variância Explicada Cumulativa:\", cumulative_explained_variance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7uA3ttI2ZQe",
        "outputId": "7237f5d0-1209-44b0-b209-798278143274"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Variância Explicada por Componente: [0.52034189 0.47965811]\n",
            "Variância Explicada Cumulativa: [0.52034189 1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotar variância explicada\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, align='center', label='Variância Individual')\n",
        "plt.step(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, where='mid', label='Variância Cumulativa')\n",
        "plt.ylabel('Ratio de Variância Explicada')\n",
        "plt.xlabel('Componente Principal')\n",
        "plt.legend(loc='best')\n",
        "plt.title('Variância Explicada pelos Componentes Principais')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mwts8Qag1OF6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "1d2834fe-3151-4fb3-b8ea-facedfecd248"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Reinstanciar PCA com número desejado de componentes (e.g., 1, baseado no gráfico ou % desejada)\n",
        "pca_final = PCA(n_components=1)\n",
        "pca_final.fit(X_scaled) # Ajustar novamente (ou usar o já ajustado e selecionar)\n",
        "\n",
        "# 6. Transformar dados de treino e teste\n",
        "X_pca_train = pca_final.transform(X_scaled)\n",
        "# Se houvesse X_test_scaled: X_pca_test = pca_final.transform(X_test_scaled)\n",
        "\n",
        "print(f\"\\nDados Originais Escalados (Shape: {X_scaled.shape}):\\n\", X_scaled[:5])\n",
        "print(f\"\\nDados Transformados por PCA (Shape: {X_pca_train.shape}):\\n\", X_pca_train[:5])\n",
        "print(\"\\nComponentes Principais (Autovetores):\\n\", pca_final.components_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ4yE5Uf2b7X",
        "outputId": "38f1d98e-4ce5-41ef-cf9f-dca622294dbe"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dados Originais Escalados (Shape: (100, 2)):\n",
            " [[ 0.4799911  -0.80495929]\n",
            " [ 0.12758337 -1.00821351]\n",
            " [-1.60874596 -0.38914033]\n",
            " [ 0.70185794 -1.24121543]\n",
            " [ 0.43837217  1.04612573]]\n",
            "\n",
            "Dados Transformados por PCA (Shape: (100, 1)):\n",
            " [[ 0.90859714]\n",
            " [ 0.80312968]\n",
            " [-0.86239141]\n",
            " [ 1.37396035]\n",
            " [-0.42974666]]\n",
            "\n",
            "Componentes Principais (Autovetores):\n",
            " [[ 0.70710678 -0.70710678]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Quais suas vantagens e limitações?**\n",
        "    *   **Vantagens:** Redução de dimensionalidade eficaz preservando variância. Remove multicolinearidade (PCs são ortogonais). Pode reduzir ruído (componentes de baixa variância podem ser ruído). Útil para visualização (2 ou 3 PCs).\n",
        "    *   **Limitações:** **Perda de interpretabilidade:** Os PCs são **combinações lineares das features originais** e raramente têm um significado de negócio claro. **Sensível à escala das features** (requer escalonamento prévio). Assume que variância alta = importância, o que nem sempre é verdade (o sinal pode estar em componentes de baixa variância). É não supervisionado (ignora a variável target).\n",
        "    *   **Precauções:** **Sempre escalone os dados antes do PCA.** Ajuste (`fit`) apenas no treino. A escolha de `n_components` é crucial (use o gráfico de variância explicada - \"cotovelo\").\n",
        "*   **Quais as principais dicas práticas?**\n",
        "    *   **Use o gráfico da variância explicada cumulativa** para decidir quantos componentes manter (procure um \"cotovelo\" ou um limiar como 90%, 95%, 99% da variância).\n",
        "    *   Ótimo para **visualizar dados de alta dimensão** em 2D ou 3D (plotando PC1 vs PC2).\n",
        "    *   Pode **melhorar o desempenho de algoritmos sensíveis à multicolinearidade** (como Regressão Linear) ou à \"maldição da dimensionalidade\" (como KNN).\n",
        "    *   **Não use PCA se a interpretabilidade das features originais for muito importante. Nesse caso, prefira Feature Selection.**"
      ],
      "metadata": {
        "id": "PldHjEJR1RPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Referências:\n",
        "- [StatQuest: Análise de Componentes Principais (PCA), Passo a Passo](https://www.youtube.com/watch?v=FgakZw6K1QQ&t=144s)\n",
        "- [Reconhecimento de Padrões - USP - (PCA) Análise de Componentes Principais](https://www.youtube.com/watch?v=2MySwcqdi2A&list=PLXIcG57UWcHteKpa4IyLqUg46n_t2Bn_F)"
      ],
      "metadata": {
        "id": "1T9GVvV9JY-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. LDA (Linear Discriminant Analysis - Análise de Discriminante Linear)**\n"
      ],
      "metadata": {
        "id": "Rd2O7kHO3gcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **O que é?**\n",
        "    *   **Motivação:** Reduzir a dimensionalidade especificamente para problemas de **classificação**, encontrando um subespaço que maximiza a separabilidade entre as classes. É uma técnica **supervisionada**.\n",
        "    *   **Proposta:** Encontrar eixos (Discriminantes Lineares - LDs) que **maximizam a razão entre a variância *entre* classes e a variância *dentro* das classes**.\n",
        "*   **Qual o seu funcionamento?**\n",
        "    *   **Pressupostos:** Assume que os dados em cada **classe seguem uma distribuição Gaussiana**. Assume que as **classes têm matrizes de covariância idênticas** (embora possa funcionar razoavelmente bem mesmo se violado). **Funciona melhor com relações lineares**. Escalonamento é frequentemente recomendado.\n",
        "    *   **Lógica:**\n",
        "        1.  Calcula a matriz de dispersão *dentro* das classes (Within-class scatter matrix, Sw). Mede o quão espalhados estão os pontos dentro de cada classe.\n",
        "        2.  Calcula a matriz de dispersão *entre* as classes (Between-class scatter matrix, Sb). Mede o quão separadas estão as médias das classes.\n",
        "        3.  Resolve um problema de autovalores/autovetores generalizado para a matriz `inv(Sw) * Sb`.\n",
        "        4.  Os autovetores correspondentes aos maiores autovalores são os Discriminantes Lineares (LDs) que maximizam a separabilidade.\n",
        "        5.  Ordena os LDs pelos autovalores.\n",
        "        6.  Seleciona os `k` primeiros LDs. O número máximo de LDs é `min(n_features, n_classes - 1)`.\n",
        "        7.  Projeta os dados originais nos `k` LDs selecionados.\n",
        "*   **Como utilizá-lo? (Python com `scikit-learn`)**\n",
        "  \n"
      ],
      "metadata": {
        "id": "hTnffYhrEWhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "A17JcC0WvHbj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Dados de exemplo (Numéricas: N1, N2, N_QuaseConstante; Categóricas: C1, C2)\n",
        "data = {\n",
        "    'N1': np.random.rand(100) * 10,\n",
        "    'N2': np.random.rand(100) * 5 + 2,\n",
        "    'N_QuaseConstante': [1]*98 + [1.1, 1.2],\n",
        "    'C1': np.random.choice(['A', 'B', 'C', 'D'], 100),\n",
        "    'C2': np.random.choice(['X', 'Y'], 100),\n",
        "    'Target_Class': np.random.randint(0, 3, 100), # Target Categórico (0, 1, 2)\n",
        "    'Target_Reg': np.random.rand(100) * 50 # Target Numérico\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "display(df.head())\n",
        "display(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "9183862c-2f38-41d6-a90d-7015950ce809",
        "id": "mclMktp-vOIn"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         N1        N2  N_QuaseConstante C1 C2  Target_Class  Target_Reg\n",
              "0  7.588773  3.565870               1.0  B  X             2   40.556147\n",
              "1  5.184875  5.562157               1.0  A  Y             2   29.480357\n",
              "2  4.377040  4.796055               1.0  C  Y             2    9.702298\n",
              "3  5.776577  5.779878               1.0  D  X             0   28.648992\n",
              "4  5.945869  2.067002               1.0  B  Y             0   11.063211"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7f0d882b-f36f-46d7-8b66-522b8f411a10\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>N1</th>\n",
              "      <th>N2</th>\n",
              "      <th>N_QuaseConstante</th>\n",
              "      <th>C1</th>\n",
              "      <th>C2</th>\n",
              "      <th>Target_Class</th>\n",
              "      <th>Target_Reg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.588773</td>\n",
              "      <td>3.565870</td>\n",
              "      <td>1.0</td>\n",
              "      <td>B</td>\n",
              "      <td>X</td>\n",
              "      <td>2</td>\n",
              "      <td>40.556147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.184875</td>\n",
              "      <td>5.562157</td>\n",
              "      <td>1.0</td>\n",
              "      <td>A</td>\n",
              "      <td>Y</td>\n",
              "      <td>2</td>\n",
              "      <td>29.480357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.377040</td>\n",
              "      <td>4.796055</td>\n",
              "      <td>1.0</td>\n",
              "      <td>C</td>\n",
              "      <td>Y</td>\n",
              "      <td>2</td>\n",
              "      <td>9.702298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.776577</td>\n",
              "      <td>5.779878</td>\n",
              "      <td>1.0</td>\n",
              "      <td>D</td>\n",
              "      <td>X</td>\n",
              "      <td>0</td>\n",
              "      <td>28.648992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.945869</td>\n",
              "      <td>2.067002</td>\n",
              "      <td>1.0</td>\n",
              "      <td>B</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>11.063211</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f0d882b-f36f-46d7-8b66-522b8f411a10')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7f0d882b-f36f-46d7-8b66-522b8f411a10 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7f0d882b-f36f-46d7-8b66-522b8f411a10');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-605e4c97-5106-4ece-9054-90329c3b63cf\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-605e4c97-5106-4ece-9054-90329c3b63cf')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-605e4c97-5106-4ece-9054-90329c3b63cf button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"N1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.185486182317183,\n        \"min\": 4.37704012274314,\n        \"max\": 7.588773162619004,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          5.18487538072682,\n          5.945868936998952,\n          4.37704012274314\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"N2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.544319543252772,\n        \"min\": 2.0670019356928786,\n        \"max\": 5.779877888994756,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          5.5621568577966345,\n          2.0670019356928786,\n          4.796054632083845\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"N_QuaseConstante\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"C1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"A\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"C2\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Y\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target_Class\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target_Reg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13.204904266023705,\n        \"min\": 9.702297527829312,\n        \"max\": 40.55614745657553,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          29.480357304394815\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(100, 7)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (usando X e y categórico do exemplo anterior) ...\n",
        "X = df[['N1', 'N2']] # Features numéricas\n",
        "y = df['Target_Class'] # Target categórico"
      ],
      "metadata": {
        "id": "psDixqBHvOIq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Escalonar os dados (recomendado para LDA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 2. Instanciar LDA\n",
        "# n_components não pode ser maior que min(n_features, n_classes - 1)\n",
        "n_classes = len(np.unique(y))\n",
        "n_features = X.shape[1]\n",
        "max_components = min(n_features, n_classes - 1)\n",
        "print(f\"\\nNúmero máximo de componentes LDA: {max_components}\")\n",
        "\n",
        "# Se max_components >= 1, podemos reduzir\n",
        "if max_components >= 1:\n",
        "  lda = LinearDiscriminantAnalysis(n_components=max_components) # Usar o máximo possível aqui\n",
        "\n",
        "  # 3. Ajustar LDA nos dados de treino (requer X e y)\n",
        "  lda.fit(X_scaled, y)\n",
        "\n",
        "  # 4. Transformar dados de treino e teste\n",
        "  X_lda_train = lda.transform(X_scaled)\n",
        "  # Se houvesse X_test_scaled: X_lda_test = lda.transform(X_test_scaled)\n",
        "\n",
        "  print(f\"\\nDados Originais Escalados (Shape: {X_scaled.shape}):\\n\", X_scaled[:5])\n",
        "  print(f\"\\nDados Transformados por LDA (Shape: {X_lda_train.shape}):\\n\", X_lda_train[:5])\n",
        "  print(\"\\nVariância explicada pelos discriminantes:\", lda.explained_variance_ratio_)\n",
        "\n",
        "  # Visualização (se n_components=2)\n",
        "  if max_components >= 2:\n",
        "        lda_plot = LinearDiscriminantAnalysis(n_components=2).fit_transform(X_scaled, y)\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.scatter(lda_plot[:, 0], lda_plot[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
        "        plt.xlabel('LD1')\n",
        "        plt.ylabel('LD2')\n",
        "        plt.title('LDA - Projeção em 2 Componentes')\n",
        "        plt.show()\n",
        "  elif max_components == 1:\n",
        "        plt.figure(figsize=(6, 2))\n",
        "        plt.scatter(X_lda_train[:, 0], np.zeros_like(X_lda_train[:, 0]), c=y, cmap='viridis', edgecolor='k')\n",
        "        plt.xlabel('LD1')\n",
        "        plt.yticks([])\n",
        "        plt.title('LDA - Projeção em 1 Componente')\n",
        "        plt.show()\n",
        "\n",
        "else:\n",
        "  print(\"\\nLDA não aplicável para redução de dimensionalidade (n_classes <= 2 e n_features=1 ou n_classes=1)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "id": "Pqi1CLDFu8OD",
        "outputId": "d0e98317-a2a2-425e-ae46-6ffed80f86d8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Número máximo de componentes LDA: 2\n",
            "\n",
            "Dados Originais Escalados (Shape: (100, 2)):\n",
            " [[ 0.78690862 -0.56992677]\n",
            " [-0.12959163  0.7774762 ]\n",
            " [-0.43758361  0.26039191]\n",
            " [ 0.09599802  0.92442804]\n",
            " [ 0.16054166 -1.58159501]]\n",
            "\n",
            "Dados Transformados por LDA (Shape: (100, 2)):\n",
            " [[-1.05828747 -0.08036496]\n",
            " [ 0.67483899 -0.46540611]\n",
            " [ 0.54709777  0.08427105]\n",
            " [ 0.59868468 -0.7057304 ]\n",
            " [-1.28883731  1.00956113]]\n",
            "\n",
            "Variância explicada pelos discriminantes: [0.94559493 0.05440507]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Quais suas vantagens e limitações?**\n",
        "    *   **Vantagens:** É supervisionado, usa informações das classes para encontrar a melhor projeção para separabilidade. **Muitas vezes supera PCA em tarefas de classificação se as suposições forem razoavelmente atendidas**. Pode funcionar bem mesmo com poucos dados por classe.\n",
        "    *   **Limitações:** Limitado a `n_classes - 1` componentes, o que pode ser uma redução muito drástica se `n_classes` for pequeno. **Assume normalidade Gaussiana e covariâncias iguais** (sensível a violações dessas premissas). Focado em separabilidade linear.\n",
        "    *   **Precauções:** **Escalonar os dados é recomendado**. Verifique o número de classes e features para determinar o `n_components` máximo. LDA pode ser usado diretamente como classificador também.\n",
        "*   **Quais as principais dicas práticas?**\n",
        "    *   **Use LDA quando o objetivo principal for classificação** e você quiser reduzir a dimensionalidade levando em conta as classes.\n",
        "    *   É uma ótima alternativa ao PCA **quando as classes são linearmente separáveis** (ou aproximadamente).\n",
        "    *   **O número máximo de componentes é uma limitação importante a considerar**. Se `n_classes=2`, LDA produzirá no máximo 1 dimensão.\n",
        "    *   Pode ser muito eficaz como passo de pré-processamento antes de aplicar outros classificadores."
      ],
      "metadata": {
        "id": "0UBfxCjAu5WE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusão e Considerações Finais"
      ],
      "metadata": {
        "id": "WI4GnhYdF2b2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "A Preparação de Dados e a Engenharia de Features são tanto uma ciência quanto uma arte. Não existe uma \"receita de bolo\" única que funcione para todos os problemas. A escolha das técnicas corretas depende profundamente:\n",
        "\n",
        "1.  **Do Problema de Negócio:** Qual o objetivo? Interpretabilidade é crucial? Qual a métrica de sucesso?\n",
        "2.  **Dos Dados:** Qual o volume? Quais os tipos de dados (numéricos, categóricos, texto, imagem)? Qual a qualidade (outliers, missing values)? Qual a cardinalidade das features categóricas?\n",
        "3.  **Do Modelo Escolhido:** Algoritmos diferentes têm sensibilidades diferentes à escala, multicolinearidade, dimensionalidade e tipos de features.\n",
        "\n",
        "**Dicas Gerais \"Game Changer\":**\n",
        "\n",
        "1.  **Iteração é a Chave:** Raramente você acertará na primeira tentativa. Experimente diferentes combinações de escalonamento, encoding, seleção e redução. Use pipelines (`sklearn.pipeline.Pipeline`) para organizar e automatizar esses passos, facilitando a experimentação e evitando data leakage.\n",
        "2.  **Visualização é Poder:** Antes, durante e depois do pré-processamento, visualize seus dados. Histogramas, boxplots, scatter plots, matrizes de correlação, projeções PCA/LDA ajudam a entender os dados e o efeito das transformações.\n",
        "3.  **Cuidado com Data Leakage:** A regra de ouro é: qualquer estatística ou parâmetro usado para transformar os dados (médias, desvios padrão, min/max, categorias, componentes PCA/LDA, médias de Target Encoding) deve ser calculado **APENAS** no conjunto de treino e depois aplicado de forma consistente aos conjuntos de validação e teste. Validação cruzada deve ser feita *corretamente*, com o pré-processing dentro de cada fold se ele depender dos dados (como Target Encoding ou PCA ajustado no fold).\n",
        "4.  **Conhecimento de Domínio:** Converse com especialistas da área. Eles podem fornecer insights valiosos sobre quais features são importantes, como combinar features existentes (Engenharia de Features manual) ou quais relações esperar.\n",
        "5.  **Simplicidade Primeiro:** Comece com abordagens mais simples (StandardScaler, OHE, Filtros básicos). Se o desempenho não for suficiente, introduza técnicas mais complexas (RobustScaler, Target Encoding, Wrappers, PCA/LDA), mas sempre avaliando o trade-off entre complexidade e ganho de performance.\n",
        "\n",
        "Dominar a preparação de dados e a engenharia de features é o que separa um modelo mediano de um modelo de alto desempenho e confiável no mundo real. Invista tempo aqui, pois o retorno é quase sempre garantido!"
      ],
      "metadata": {
        "id": "6RC6aZw6EaSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apêndice"
      ],
      "metadata": {
        "id": "hmzlJXgaO7S4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A. Interação entre Features e Limitação dos métodos de Filtro"
      ],
      "metadata": {
        "id": "zEpIHqraO91h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ótima pergunta! Explicar o que significa \"ignorar interações entre features\" é fundamental para entender as limitações dos Métodos Filtro na seleção de features.\n",
        "\n",
        "Vamos detalhar:\n",
        "\n",
        "**O que é uma \"Interação entre Features\"?**\n",
        "\n",
        "Uma interação ocorre quando o efeito de uma feature (variável preditora) sobre a variável alvo (target) **depende do valor de outra feature**. Em outras palavras, o impacto de uma variável não é constante, mas muda de acordo com o contexto fornecido por outra(s) variável(is). As features, nesse caso, não atuam de forma isolada; seu poder preditivo combinado é diferente (geralmente maior, mas pode ser menor) do que a soma de seus poderes preditivos individuais.\n",
        "\n",
        "**Exemplos de Interação:**\n",
        "\n",
        "1.  **Marketing:** O efeito do gasto em publicidade na TV (`Gasto_TV`) nas vendas (`Vendas`) pode ser muito maior se houver também um gasto significativo em publicidade online (`Gasto_Online`) ocorrendo simultaneamente. Sozinho, o `Gasto_TV` pode ter um efeito modesto, mas em *interação* com `Gasto_Online`, o efeito é amplificado.\n",
        "2.  **Medicina:** Um medicamento A pode ter pouco efeito em reduzir a pressão arterial. Um medicamento B também pode ter pouco efeito. No entanto, tomados *juntos*, eles podem ter um efeito sinérgico e reduzir significativamente a pressão arterial. A eficácia de A *depende* da presença de B.\n",
        "3.  **Geometria Simples:** Imagine que você quer prever se um ponto (X, Y) está dentro de um quadrado unitário (0 < X < 1 E 0 < Y < 1).\n",
        "    *   Saber apenas que `0 < X < 1` não é suficiente para garantir que o ponto está dentro do quadrado.\n",
        "    *   Saber apenas que `0 < Y < 1` também não é suficiente.\n",
        "    *   É a *interação* (a condição LÓGICA \"E\") entre `X` e `Y` que define se o ponto está dentro. Uma feature sozinha tem poder preditivo limitado, mas juntas (em interação) elas definem perfeitamente a classe.\n",
        "\n",
        "**Como os Métodos Filtro Ignoram Interações:**\n",
        "\n",
        "Os Métodos Filtro avaliam as features com base em suas propriedades estatísticas intrínsecas, geralmente de forma **univariada** (uma feature de cada vez em relação ao target) ou, no máximo, **bivariada** (comparando pares de features, como na correlação, mas focando em *redundância*, não em *poder preditivo conjunto*).\n",
        "\n",
        "*   **Avaliação Univariada:** Métodos como ANOVA F-test, Chi-Quadrado ou Informação Mútua calculam uma pontuação para cada feature *individualmente*, medindo sua relação direta com o target. Eles não consideram como essa relação pode mudar na presença de outras features. No exemplo do quadrado, tanto X quanto Y individualmente teriam uma pontuação relativamente baixa em relação ao target \"dentro/fora\", pois nenhum deles sozinhos define a fronteira.\n",
        "*   **Avaliação de Redundância:** Métodos baseados em correlação olham para pares de features (`X1`, `X2`) para ver se elas carregam informações similares (redundantes). Eles *não* avaliam se a *combinação* de `X1` e `X2` (quando não são redundantes) tem um poder preditivo maior do que a soma de suas partes.\n",
        "\n",
        "**Consequências de Ignorar Interações:**\n",
        "\n",
        "1.  **Descarte de Features Valiosas:** Uma feature pode parecer fraca ou irrelevante quando avaliada isoladamente por um método filtro (baixa correlação com o target, baixo score ANOVA/MI). No entanto, essa mesma feature pode ser crucial quando *combinada* com outra. O método filtro pode descartá-la erroneamente.\n",
        "2.  **Seleção de Subconjunto Subótimo:** O conjunto final de features selecionado pelo filtro pode não ser o melhor para um modelo que *é* capaz de capturar interações (como árvores de decisão, Random Forests, Gradient Boosting, ou modelos lineares com termos de interação explícitos).\n",
        "3.  **Perda de Poder Preditivo:** Se as interações são importantes para modelar o fenômeno subjacente, ignorá-las levará a um modelo menos preciso.\n",
        "\n",
        "**Em Resumo:**\n",
        "\n",
        "Dizer que os métodos filtro \"ignoram interações entre features\" significa que eles avaliam o mérito de cada feature **isoladamente** (ou em pares para redundância), sem considerar a possibilidade de que o verdadeiro valor preditivo de uma feature só se manifeste ou seja amplificado **em conjunto** com outras features. Eles não conseguem detectar a \"sinergia\" ou as dependências contextuais entre as variáveis na previsão do alvo. É por isso que métodos Wrapper e Embedded, que avaliam features no contexto de um modelo específico, são geralmente melhores em encontrar subconjuntos de features que funcionam bem juntas."
      ],
      "metadata": {
        "id": "LG9fB0oJPIBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### B. DeepDive no código dos Método de Filtros"
      ],
      "metadata": {
        "id": "vJy1pey4UwdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, vamos mergulhar nos detalhes do trecho de código que usa `SelectKBest`. Este é um exemplo clássico de **Método Filtro** para seleção de features, onde as features são avaliadas com base em testes estatísticos univariados (ou seja, cada feature é avaliada individualmente em relação à variável alvo).\n",
        "\n",
        "O objetivo geral aqui é **selecionar um número pré-definido (`k`) das features que têm a relação estatística mais forte com a variável alvo (target)**.\n",
        "\n",
        "```python\n",
        "# --- Filtros baseados no Target (SelectKBest) ---\n",
        "# Importações relevantes já feitas: SelectKBest, f_classif, chi2, mutual_info_classif, LabelEncoder\n",
        "# Dados de exemplo (já criados): df com features N1, N2 (numéricas), C1, C2 (categóricas), Target_Class (categórica)\n",
        "\n",
        "# --- Cenário 1: Features Numéricas vs Target Categórico (ANOVA F-test) ---\n",
        "\n",
        "# 1. Escolher a métrica e o número de features (k)\n",
        "#    SelectKBest: Classe que seleciona features com base nos 'k' maiores scores.\n",
        "#    score_func=f_classif: Especifica a função de pontuação a ser usada.\n",
        "#        f_classif: Calcula o teste F de ANOVA (Análise de Variância).\n",
        "#                   Este teste é adequado para:\n",
        "#                       - Features de entrada (X): Numéricas\n",
        "#                       - Variável Alvo (y): Categórica\n",
        "#                   Ele mede se as médias da feature numérica são significativamente\n",
        "#                   diferentes entre os grupos definidos pelo target categórico.\n",
        "#                   Um valor F alto e um p-valor baixo sugerem que a feature ajuda\n",
        "#                   a discriminar as classes do target.\n",
        "#    k=1: Queremos selecionar apenas a UMA (k=1) melhor feature numérica\n",
        "#         com base no score F de ANOVA.\n",
        "selector_f_classif = SelectKBest(score_func=f_classif, k=1)\n",
        "\n",
        "# 2. Aplicar o seletor aos dados\n",
        "#    df[['N1', 'N2']]: Selecionamos apenas as features NUMÉRICAS de entrada.\n",
        "#    df['Target_Class']: Fornecemos a variável alvo CATEGÓRICA.\n",
        "#    .fit(X, y): Este método faz o seguinte:\n",
        "#        a. Para CADA feature em X ('N1' e 'N2'):\n",
        "#           Calcula o score usando a função especificada (f_classif) em relação a 'y'.\n",
        "#           Neste caso, calcula o F-score de ANOVA para 'N1' vs 'Target_Class'\n",
        "#           e para 'N2' vs 'Target_Class'.\n",
        "#        b. Armazena internamente os scores calculados para todas as features.\n",
        "#        c. Identifica qual(is) feature(s) tem(êm) os 'k' maiores scores.\n",
        "#    Importante: O método `fit` NÃO modifica os dados originais (X ou y).\n",
        "#                Ele apenas calcula e armazena os resultados da análise.\n",
        "selector_f_classif.fit(df[['N1', 'N2']], df['Target_Class'])\n",
        "\n",
        "# 3. Analisar os resultados\n",
        "#    .get_feature_names_out(['N1', 'N2']): Retorna os nomes das features que foram\n",
        "#                                          SELECIONADAS (as 'k' melhores).\n",
        "#                                          Passar os nomes originais ajuda a obter\n",
        "#                                          nomes significativos na saída.\n",
        "print(\"\\nMelhor feature numérica para Target_Class (ANOVA):\",\n",
        "      selector_f_classif.get_feature_names_out(['N1', 'N2']))\n",
        "\n",
        "#    .scores_: Atributo que armazena os scores calculados para TODAS as features\n",
        "#              que foram avaliadas durante o `fit`, na mesma ordem em que foram passadas.\n",
        "#              Permite ver a pontuação de cada feature, não apenas da(s) selecionada(s).\n",
        "print(\"Scores ANOVA:\", selector_f_classif.scores_)\n",
        "\n",
        "# --- Cenário 2: Features Categóricas vs Target Categórico (Chi2) ---\n",
        "\n",
        "# 4. Pré-processamento NECESSÁRIO para Chi2\n",
        "#    O teste Qui-Quadrado (Chi2) opera sobre tabelas de contingência, comparando\n",
        "#    frequências observadas com frequências esperadas sob a hipótese de independência.\n",
        "#    Ele requer que as entradas sejam NUMÉRICAS e NÃO-NEGATIVAS.\n",
        "#    Nossas features 'C1' e 'C2' são strings ('A', 'B', 'X', 'Y', etc.).\n",
        "#    LabelEncoder: Converte cada categoria string em um número inteiro (0, 1, 2...).\n",
        "#                  É uma forma rápida de obter a representação numérica necessária.\n",
        "#                  NOTA: Embora introduza uma ordem artificial (que Chi2 não usa),\n",
        "#                        é comum usá-lo aqui para aplicar o teste em filtros.\n",
        "#                        OneHotEncoding também funcionaria, mas geraria mais colunas\n",
        "#                        e exigiria aplicar Chi2 a cada coluna binária resultante.\n",
        "le = LabelEncoder()\n",
        "df['C1_Encoded'] = le.fit_transform(df['C1'])\n",
        "df['C2_Encoded'] = le.fit_transform(df['C2'])\n",
        "\n",
        "# 5. Escolher a métrica e k para features categóricas\n",
        "#    SelectKBest: Mesma classe de antes.\n",
        "#    score_func=chi2: Especifica a função de pontuação Qui-Quadrado.\n",
        "#        chi2: Adequado para:\n",
        "#                - Features de entrada (X): Categóricas (codificadas como numéricas não-negativas)\n",
        "#                - Variável Alvo (y): Categórica\n",
        "#              Testa a hipótese nula de que a feature e o target são independentes.\n",
        "#              Um valor Chi2 alto e um p-valor baixo sugerem uma dependência\n",
        "#              entre a feature e o target (ou seja, a feature é relevante).\n",
        "#    k=1: Queremos selecionar a UMA melhor feature categórica (codificada).\n",
        "selector_chi2 = SelectKBest(score_func=chi2, k=1)\n",
        "\n",
        "# 6. Aplicar o seletor aos dados CATEGÓRICOS CODIFICADOS\n",
        "#    df[['C1_Encoded', 'C2_Encoded']]: Usamos as versões NUMÉRICAS das features categóricas.\n",
        "#    df['Target_Class']: O target ainda é o mesmo (categórico).\n",
        "#    .fit(X, y): Calcula o score Chi2 para 'C1_Encoded' vs 'Target_Class'\n",
        "#                e para 'C2_Encoded' vs 'Target_Class'. Armazena os scores\n",
        "#                e identifica a feature com o maior score (top k=1).\n",
        "selector_chi2.fit(df[['C1_Encoded', 'C2_Encoded']], df['Target_Class'])\n",
        "\n",
        "# 7. Analisar os resultados para Chi2\n",
        "#    .get_feature_names_out(['C1_Encoded', 'C2_Encoded']): Mostra qual feature codificada foi selecionada.\n",
        "print(\"\\nMelhor feature categórica para Target_Class (Chi2):\",\n",
        "       selector_chi2.get_feature_names_out(['C1_Encoded', 'C2_Encoded']))\n",
        "#    .scores_: Mostra os scores Chi2 para ambas as features codificadas.\n",
        "print(\"Scores Chi2:\", selector_chi2.scores_)\n",
        "\n",
        "# --- Cenário 3: Informação Mútua (Exemplo de uso direto da função de score) ---\n",
        "\n",
        "# 8. Calcular Scores de Informação Mútua\n",
        "#    mutual_info_classif: Calcula a Informação Mútua entre cada feature e o target discreto.\n",
        "#        Informação Mútua (MI): Mede a redução na incerteza sobre o target 'y'\n",
        "#                                ao conhecer o valor da feature 'X'.\n",
        "#                                Vantagens:\n",
        "#                                   - Captura dependências NÃO LINEARES.\n",
        "#                                   - Pode ser usada para features numéricas ou categóricas.\n",
        "#                                   - Valor >= 0. Zero significa independência.\n",
        "#    df[['N1', 'N2', 'C1_Encoded', 'C2_Encoded']]: Passamos todas as features (numéricas e categóricas codificadas).\n",
        "#    df['Target_Class']: O target categórico.\n",
        "#    discrete_features=[False, False, True, True]: Um *hint* importante para a função.\n",
        "#        Indica quais colunas em X devem ser tratadas como discretas/categóricas.\n",
        "#        'N1', 'N2' são contínuas (False).\n",
        "#        'C1_Encoded', 'C2_Encoded' são discretas (True).\n",
        "#        Isso ajuda o algoritmo a usar os estimadores corretos de MI.\n",
        "#    A função retorna diretamente um array com os scores MI para cada feature.\n",
        "mi_scores = mutual_info_classif(df[['N1', 'N2', 'C1_Encoded', 'C2_Encoded']], df['Target_Class'], discrete_features=[False, False, True, True])\n",
        "\n",
        "# 9. Exibir os scores MI\n",
        "#    Colocamos os scores em uma Série Pandas para melhor visualização com os nomes das features.\n",
        "print(\"\\nScores de Informação Mútua (vs Target_Class):\\n\", pd.Series(mi_scores, index=['N1', 'N2', 'C1_Encoded', 'C2_Encoded']))\n",
        "#    NOTA: Aqui, apenas calculamos os scores. Se quiséssemos usar MI dentro do\n",
        "#          SelectKBest, faríamos: SelectKBest(score_func=mutual_info_classif, k=...)\n",
        "#          e depois o `.fit()`. Usar a função diretamente é útil para apenas\n",
        "#          ver os scores e decidir um limiar manualmente, ou para usar em cenários\n",
        "#          onde os hints como `discrete_features` são necessários e SelectKBest\n",
        "#          pode não passá-los diretamente (embora versões recentes possam ter melhorado isso).\n",
        "```\n",
        "\n",
        "**Em Resumo, o que o código faz:**\n",
        "\n",
        "1.  **Instancia `SelectKBest`**: Prepara um objeto para selecionar as `k` melhores features.\n",
        "2.  **Especifica `score_func`**: Define *como* a \"qualidade\" ou \"relevância\" de uma feature será medida em relação ao target. A escolha da função (`f_classif`, `chi2`, `mutual_info_classif`, etc.) depende dos **tipos de dados** da feature e do target.\n",
        "3.  **Especifica `k`**: Define *quantas* features serão selecionadas.\n",
        "4.  **Executa `.fit(X, y)`**: Calcula os scores para todas as features em `X` usando a `score_func` especificada em relação a `y` e armazena esses scores.\n",
        "5.  **Interpreta os Resultados**: Usa métodos como `.get_feature_names_out()` para ver as features selecionadas e o atributo `.scores_` para inspecionar as pontuações calculadas para todas as features avaliadas.\n",
        "6.  **(Implícito)** O próximo passo *seria* usar `.transform(X)` no objeto `selector` ajustado (`fit`) para criar um novo conjunto de dados contendo apenas as `k` features selecionadas. Ex: `X_new = selector_f_classif.transform(df[['N1', 'N2']])`.\n",
        "\n",
        "Este processo permite filtrar rapidamente as features que, individualmente, mostram a maior relação estatística com a variável alvo, antes de passar para etapas de modelagem mais complexas."
      ],
      "metadata": {
        "id": "NJKtavjSU01g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### C. DeepDive no RFE com Regressão Logística"
      ],
      "metadata": {
        "id": "hvPJi35S2AlN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**O Cenário:**\n",
        "\n",
        "Você tem o código:\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Suponha que X_train e y_train existem\n",
        "# X_train tem múltiplas features (colunas)\n",
        "# y_train é o target (geralmente binário para Regressão Logística)\n",
        "\n",
        "estimator_lr = LogisticRegression(solver='liblinear') # O modelo base\n",
        "selector_rfe = RFE(estimator=estimator_lr, n_features_to_select=1, step=1)\n",
        "selector_rfe.fit(X_train, y_train)\n",
        "\n",
        "# Resultado de interesse: Qual feature foi selecionada (se n_features_to_select=1)?\n",
        "print(\"Features selecionadas por RFE:\", X_train.columns[selector_rfe.support_])\n",
        "print(\"Ranking das features:\", selector_rfe.ranking_)\n",
        "```\n",
        "\n",
        "**O que o RFE faz com a Regressão Logística?**\n",
        "\n",
        "RFE é um método **Wrapper** de seleção de features. Isso significa que ele usa um modelo de aprendizado de máquina (o `estimator`) como parte do seu processo para avaliar a importância das features. Ele funciona de forma **recursiva e eliminatória**:\n",
        "\n",
        "1.  **Modelo Base:** O RFE precisa de um modelo que, após ser treinado, forneça alguma medida de importância para cada feature. A Regressão Logística é adequada porque, ao ser treinada, ela calcula **coeficientes (pesos)** para cada feature de entrada.\n",
        "\n",
        "2.  **Interpretação dos Coeficientes:** Na Regressão Logística, o coeficiente de uma feature indica o quanto o *log-odds* da classe positiva aumenta (ou diminui, se negativo) para cada unidade de aumento na feature, mantendo as outras constantes. A **magnitude (valor absoluto)** desses coeficientes é frequentemente usada como um proxy para a importância da feature no modelo:\n",
        "    *   Um coeficiente com **grande valor absoluto** (positivo ou negativo) sugere que a feature tem uma forte influência na previsão do modelo.\n",
        "    *   Um coeficiente com **valor absoluto próximo de zero** sugere que a feature tem pouca influência na previsão *daquele modelo específico treinado com aquele conjunto de features*.\n",
        "\n",
        "3.  **O Processo Recursivo de Eliminação:**\n",
        "    *   **Passo Inicial (Iteração 1):**\n",
        "        *   O RFE pega **todas** as features iniciais em `X_train`.\n",
        "        *   Ele treina o modelo `LogisticRegression` (`estimator_lr`) usando todas essas features e o target `y_train`.\n",
        "        *   Após o treino, ele extrai os **coeficientes** aprendidos pelo modelo (`estimator_lr.coef_`).\n",
        "        *   Ele calcula o **valor absoluto** de cada coeficiente.\n",
        "        *   Ele identifica a feature que possui o **menor valor absoluto de coeficiente**. Essa feature é considerada a \"menos importante\" *nesta iteração*.\n",
        "    *   **Passo de Eliminação (Iteração 1):**\n",
        "        *   A feature identificada como a menos importante é **removida permanentemente** do conjunto de features.\n",
        "        *   O `ranking_` dessa feature é definido (por exemplo, se havia 10 features, o ranking da primeira eliminada seria 10).\n",
        "    *   **Passo Recursivo (Iteração 2):**\n",
        "        *   O RFE agora tem um conjunto de features menor (todas as iniciais menos uma).\n",
        "        *   Ele **retreina** o modelo `LogisticRegression` usando **apenas as features restantes** e o target `y_train`.\n",
        "        *   Após o treino, ele extrai os novos coeficientes (que provavelmente serão diferentes dos da Iteração 1, pois o conjunto de features mudou).\n",
        "        *   Ele calcula os valores absolutos dos novos coeficientes.\n",
        "        *   Ele identifica a feature (dentre as *restantes*) com o menor valor absoluto de coeficiente.\n",
        "        *   Essa feature é removida, e seu ranking é definido (ex: 9).\n",
        "    *   **Repetição:** O processo (retreinar -> obter coeficientes -> encontrar o menor absoluto -> eliminar) continua.\n",
        "    *   **Condição de Parada:** O processo para quando o número de features restantes atinge o valor especificado em `n_features_to_select` (no seu caso, 1).\n",
        "    *   **Resultado Final:** As features que *não* foram eliminadas são consideradas as \"melhores\" ou \"selecionadas\" pelo RFE. Se `n_features_to_select=1`, apenas uma feature sobrará. O `ranking_` indicará a ordem de eliminação (1 para as selecionadas, 2 para a primeira eliminada antes das selecionadas, e assim por diante).\n",
        "\n",
        "**No seu caso específico (`n_features_to_select=1`):**\n",
        "\n",
        "O RFE treinará a Regressão Logística várias vezes. Em cada etapa, ele removerá a feature cujo coeficiente (em valor absoluto) for o menor *naquele treinamento específico*. Ele continuará removendo features até que apenas uma reste. Essa única feature restante é a que, segundo esse processo iterativo baseado nos coeficientes da Regressão Logística, foi considerada a mais importante (ou, mais precisamente, a última a ser considerada \"menos importante\").\n",
        "\n",
        "**Considerações Importantes:**\n",
        "\n",
        "*   **Escalonamento:** Os coeficientes da Regressão Logística são muito sensíveis à escala das features. Se as features tiverem escalas muito diferentes (ex: idade 20-80 vs salário 50000-200000), a feature com maior escala pode ter um coeficiente artificialmente menor, mesmo sendo importante. **É crucial escalar (padronizar ou normalizar) seus dados `X_train` *antes* de aplicar RFE com Regressão Logística** para que os coeficientes sejam comparáveis.\n",
        "*   **Multicolinearidade:** Se houver features altamente correlacionadas, os coeficientes da Regressão Logística podem se tornar instáveis. O RFE pode eliminar aleatoriamente uma das features correlacionadas, mesmo que ambas sejam importantes.\n",
        "*   **`step`:** O parâmetro `step=1` significa que apenas uma feature é removida por iteração. Você pode remover mais de uma para acelerar o processo (`step > 1`), mas geralmente `step=1` é mais granular.\n",
        "*   **Modelo Base:** O resultado do RFE depende totalmente do modelo base (`estimator`). Usar um RandomForestClassifier no RFE resultaria em um processo diferente, baseado na feature importance calculada pelo Random Forest, e não nos coeficientes da Regressão Logística.\n",
        "\n",
        "Em resumo, o RFE com Regressão Logística usa a magnitude dos coeficientes aprendidos em treinamentos sucessivos como critério para eliminar iterativamente as features consideradas menos influentes, até restar o número desejado."
      ],
      "metadata": {
        "id": "LjloZFSL2G7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### D. Deep Dive: Regularização L1 (Lasso) como Método Incorporado de Seleção de Features"
      ],
      "metadata": {
        "id": "I0izJL7aA8R_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A regularização L1, mais conhecida pelo nome do algoritmo que a implementa em regressão linear (**Lasso** - Least Absolute Shrinkage and Selection Operator), é uma técnica poderosa e elegante que atua simultaneamente como um método de **regularização** (prevenindo overfitting) e um método **incorporado (embedded)** de **seleção de features**.\n",
        "\n",
        "Vamos mergulhar nos detalhes:\n",
        "\n",
        "**1. O Problema: Overfitting e Alta Dimensionalidade**\n",
        "\n",
        "*   **Overfitting:** Modelos complexos (especialmente aqueles com muitas features, como modelos lineares com muitos coeficientes) podem se ajustar excessivamente aos dados de treinamento, capturando ruído e padrões espúrios. Isso leva a um desempenho ruim em dados não vistos.\n",
        "*   **Alta Dimensionalidade:** Quando temos um grande número de features (p >> n, ou simplesmente muitas features), os **modelos lineares** podem se tornar instáveis, difíceis de interpretar, e computacionalmente caros. Muitas dessas features podem ser redundantes ou irrelevantes.\n",
        "\n",
        "**2. A Solução: Regularização**\n",
        "\n",
        "A ideia da regularização é adicionar um termo de **penalidade à função de custo (ou objetivo) do modelo**. Essa penalidade desencoraja o modelo de atribuir pesos (coeficientes) muito grandes às features, forçando-o a ser mais \"simples\".\n",
        "\n",
        "*   **Função de Custo Padrão (Ex: Regressão Linear - Mínimos Quadrados):**\n",
        "    `Custo = Σ(y_i - ŷ_i)² = Σ(y_i - (β₀ + β₁x₁ᵢ + ... + βₚxₚᵢ))²`\n",
        "    O objetivo é encontrar os coeficientes (β) que minimizam esse erro quadrático.\n",
        "\n",
        "*   **Função de Custo com Regularização L1 (Lasso):**\n",
        "    `Custo_L1 = Σ(y_i - ŷ_i)² + α * Σ|βⱼ|` (soma para j de 1 a p, excluindo o intercepto β₀)\n",
        "\n",
        "    *   **Termo de Penalidade L1:** `α * Σ|βⱼ|` é a soma dos **valores absolutos** dos coeficientes, multiplicada por um hiperparâmetro `α` (alpha) ou `λ` (lambda).\n",
        "    *   **Hiperparâmetro `α`:** Controla a **força** da regularização.\n",
        "        *   `α = 0`: Sem regularização (voltamos aos Mínimos Quadrados comuns).\n",
        "        *   `α > 0`: Penaliza coeficientes grandes.\n",
        "        *   `α → ∞`: Penalização muito forte, força todos os coeficientes `βⱼ` (j>0) a serem zero.\n",
        "\n",
        "**3. O Mecanismo Mágico: Por que L1 Seleciona Features (Gera Esparsidade)?**\n",
        "\n",
        "Esta é a parte crucial que diferencia L1 (Lasso) de L2 (Ridge Regression).\n",
        "\n",
        "*   **A Penalidade Absoluta:** A função de valor absoluto `|βⱼ|` tem um \"ponto\" ou \"quina\" em `βⱼ = 0`. Matematicamente, sua derivada não é definida em zero (é -1 para β<0 e +1 para β>0).\n",
        "*   **O Processo de Otimização:** Durante o treinamento, o algoritmo tenta minimizar a `Custo_L1`. Isso envolve encontrar um equilíbrio entre minimizar o erro de ajuste aos dados (primeiro termo) e minimizar a penalidade (segundo termo).\n",
        "*   **O Efeito da \"Quina\":** Por causa da natureza da penalidade `|βⱼ|`, quando o algoritmo de otimização (como descida de gradiente coordenada) tenta reduzir o custo total, ele frequentemente descobre que a melhor maneira de fazer isso é definir alguns coeficientes **exatamente iguais a zero**. É \"mais barato\" (em termos de redução do custo total) zerar completamente um coeficiente de uma feature pouco útil do que apenas reduzi-lo a um valor muito pequeno (como faria a regularização L2).\n",
        "*   **Interpretação Geométrica (Comparando com L2):**\n",
        "    *   Minimizar o Custo_L1 é equivalente a minimizar o erro quadrático sujeito a uma restrição `Σ|βⱼ| ≤ t` (para algum `t` relacionado a `α`). Esta região de restrição no espaço dos coeficientes tem a forma de um **diamante** (ou hiper-diamante em dimensões maiores).\n",
        "    *   A regularização L2 (Ridge) tem uma penalidade `α * Σβⱼ²`, que corresponde a uma restrição `Σβⱼ² ≤ t`. Esta região de restrição é uma **esfera** (ou hiperesfera).\n",
        "    *   As curvas de nível da função de erro quadrático são elipses. A solução ótima ocorre onde uma elipse de erro toca a região de restrição. É muito mais provável que a elipse toque o diamante *exatamente em um vértice* (onde um ou mais coeficientes são zero) do que toque a esfera em um ponto onde um coeficiente seja exatamente zero.\n",
        "\n",
        "*   **Esparsidade:** O resultado final é um **modelo esparso**, onde muitos coeficientes são exatamente zero.\n",
        "*   **Seleção de Features:** Se um coeficiente `βⱼ` é zero, isso significa que a feature `xⱼ` correspondente **não tem impacto** no modelo final. Ela foi efetivamente **selecionada para fora** (eliminada) pelo processo de regularização L1.\n",
        "\n",
        "**4. Como Utilizar L1 para Seleção de Features na Prática (Python com `scikit-learn`)**\n",
        "\n",
        "*   **Passo 1: Escalonamento dos Dados (CRUCIAL!)**\n",
        "    *   A penalidade L1 depende diretamente da magnitude dos coeficientes. **A magnitude dos coeficientes, por sua vez, depende da escala das features**. Features com escalas maiores terão coeficientes menores (para o mesmo efeito), e vice-versa.\n",
        "    *   Para que a penalidade seja aplicada de forma justa a todas as features, **você DEVE escalonar (padronizar) suas features** antes de aplicar Lasso ou Regressão Logística com L1. `StandardScaler` é a escolha comum.\n",
        "    ```python\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test) # Usar o mesmo scaler do treino!\n",
        "    ```\n",
        "\n",
        "*   **Passo 2: Escolher o Modelo e Treinar**\n",
        "    *   **Para Regressão:** Use `Lasso` ou `LassoCV`. `LassoCV` usa validação cruzada interna para encontrar o melhor valor de `α`.\n",
        "      ```python\n",
        "      from sklearn.linear_model import LassoCV\n",
        "\n",
        "      # LassoCV testa vários alphas e escolhe o melhor via CV\n",
        "      lasso_cv = LassoCV(cv=5, random_state=42, n_jobs=-1) # cv=5 -> 5-fold CV\n",
        "      lasso_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "      print(f\"Melhor alpha encontrado: {lasso_cv.alpha_}\")\n",
        "      print(\"Coeficientes Lasso:\", lasso_cv.coef_)\n",
        "      ```\n",
        "    *   **Para Classificação:** Use `LogisticRegression` com `penalty='l1'`. É importante notar que nem todos os `solver`s suportam L1. `'liblinear'` (bom para datasets menores) e `'saga'` (bom para datasets maiores) são as opções comuns. `LogisticRegressionCV` também pode ser usada para encontrar o `C` ótimo (que é o inverso de `α`, `C=1/α`).\n",
        "      ```python\n",
        "      from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "      # C é o inverso da força de regularização (C baixo = alpha alto = regularização forte)\n",
        "      # Cs=10 testa 10 valores de C logaritmicamente espaçados\n",
        "      # solver='liblinear' suporta L1\n",
        "      logreg_l1_cv = LogisticRegressionCV(Cs=10, cv=5, penalty='l1', solver='liblinear',\n",
        "                                          random_state=42, n_jobs=-1)\n",
        "      logreg_l1_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "      print(f\"Melhor C encontrado: {logreg_l1_cv.C_[0]}\") # C_ é um array\n",
        "      # Coeficientes para cada classe (se multiclasse) ou um conjunto (se binário)\n",
        "      print(\"Coeficientes LogReg L1:\", logreg_l1_cv.coef_)\n",
        "      ```\n",
        "\n",
        "*   **Passo 3: Identificar Features Selecionadas**\n",
        "    *   Após o treinamento, as features com coeficientes **diferentes de zero** são as selecionadas.\n",
        "      ```python\n",
        "      import numpy as np\n",
        "      import pandas as pd\n",
        "\n",
        "      # Exemplo com LassoCV\n",
        "      selected_features_mask = lasso_cv.coef_ != 0\n",
        "      selected_features_names = pd.DataFrame(X_train).columns[selected_features_mask] # Assumindo X_train é DataFrame\n",
        "\n",
        "      print(\"\\nFeatures selecionadas pelo Lasso:\")\n",
        "      print(selected_features_names)\n",
        "      print(f\"Número de features selecionadas: {np.sum(selected_features_mask)}\")\n",
        "\n",
        "      # Exemplo com LogisticRegressionCV (binária)\n",
        "      selected_features_mask_logreg = logreg_l1_cv.coef_[0] != 0 # [0] para classe binária\n",
        "      selected_features_names_logreg = pd.DataFrame(X_train).columns[selected_features_mask_logreg]\n",
        "\n",
        "      print(\"\\nFeatures selecionadas pela Regressão Logística L1:\")\n",
        "      print(selected_features_names_logreg)\n",
        "      print(f\"Número de features selecionadas: {np.sum(selected_features_mask_logreg)}\")\n",
        "      ```\n",
        "\n",
        "*   **Passo 4 (Opcional): Usar como Passo de Seleção para Outro Modelo**\n",
        "    *   Você pode usar L1 puramente como um passo de *feature selection* e depois treinar um modelo diferente (talvez mais complexo ou um que não suporte L1) apenas com as features selecionadas. `SelectFromModel` é útil aqui.\n",
        "      ~~~~python\n",
        "      from sklearn.feature_selection import SelectFromModel\n",
        "      from sklearn.ensemble import RandomForestClassifier # Exemplo de modelo final\n",
        "\n",
        "      # Usar o modelo L1 já treinado (lasso_cv ou logreg_l1_cv)\n",
        "      # threshold=-np.inf garante que qualquer coeficiente não zero seja selecionado\n",
        "      # Poderia usar threshold=\"median\" ou um valor específico se quisesse ser mais agressivo\n",
        "      selector = SelectFromModel(estimator=lasso_cv, prefit=True, threshold=-np.inf) # prefit=True pois já treinamos\n",
        "\n",
        "      X_train_selected = selector.transform(X_train_scaled)\n",
        "      X_test_selected = selector.transform(X_test_scaled)\n",
        "\n",
        "      print(f\"\\nShape original: {X_train_scaled.shape}\")\n",
        "      print(f\"Shape após SelectFromModel com Lasso: {X_train_selected.shape}\")\n",
        "\n",
        "      # Agora treinar outro modelo com os dados selecionados\n",
        "      # rf = RandomForestClassifier(random_state=42)\n",
        "      # rf.fit(X_train_selected, y_train)\n",
        "      # ... avaliar rf ...\n",
        "      ~~~~\n",
        "\n",
        "**5. Vantagens da Seleção de Features com L1:**\n",
        "\n",
        "*   **Automática e Incorporada:** A seleção ocorre durante o treinamento do modelo, sem necessidade de um passo separado como nos métodos Wrapper.\n",
        "*   **Eficiência Computacional:** Geralmente mais rápido que métodos Wrapper que treinam múltiplos modelos.\n",
        "*   **Esparsidade:** Produz modelos mais simples, potencialmente mais interpretáveis e que podem generalizar melhor, especialmente em alta dimensão.\n",
        "*   **Lida com Alta Dimensionalidade:** Funciona bem mesmo quando o número de features é maior que o número de amostras (p > n).\n",
        "\n",
        "**6. Limitações e Precauções:**\n",
        "\n",
        "*   **Sensibilidade à Escala:** **Fundamental** escalar os dados antes de usar L1.\n",
        "*   **Features Correlacionadas:** Se um grupo de features é altamente correlacionado, Lasso tende a selecionar **apenas uma** delas (quase arbitrariamente) e zerar as outras. Isso pode não ser ideal se todas as features correlacionadas tiverem alguma importância. Ridge (L2) tende a encolher os coeficientes de features correlacionadas juntas, enquanto Elastic Net (combinação de L1 e L2) oferece um meio-termo.\n",
        "*   **Escolha de `α` (ou `C`):** O desempenho e o grau de esparsidade dependem fortemente da escolha do hiperparâmetro de regularização. Use validação cruzada (`LassoCV`, `LogisticRegressionCV`) para encontrar um valor ótimo.\n",
        "*   **Não Garante Melhor Modelo Preditivo:** Embora ótimo para seleção e esparsidade, um modelo Lasso pode não ter o melhor desempenho preditivo puro comparado a Ridge ou Elastic Net, especialmente se muitas features contribuem um pouco para o resultado (Lasso pode zerá-las).\n",
        "*   **Limitado a Modelos com Coeficientes:** Funciona nativamente com modelos lineares (Regressão Linear, Logística, SVM Linear). Para outros modelos (ex: árvores), a seleção é feita por outros mecanismos (como feature importance).\n",
        "\n",
        "**Conclusão:**\n",
        "\n",
        "A regularização L1 (Lasso) é uma ferramenta extremamente valiosa no arsenal de um Cientista de Dados. Ela oferece uma maneira eficiente e matematicamente elegante de combater o overfitting e realizar a seleção de features simultaneamente, simplesmente adicionando um termo de penalidade à função de custo. Ao forçar coeficientes de features menos relevantes a se tornarem exatamente zero, ela produz modelos esparsos e ajuda a identificar os preditores mais importantes em conjuntos de dados, especialmente aqueles com alta dimensionalidade. Lembre-se sempre da importância do escalonamento e do ajuste do hiperparâmetro `α` via validação cruzada."
      ],
      "metadata": {
        "id": "Miz0b79MA51j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "HtuNjkQGqdht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**A regularização lasso é útil apenas para modelos lineares ou também é útili para modelos não lineares(na perspectiva de feature selection)?**\n",
        "\n",
        "A resposta curta é: **A regularização Lasso (L1) é inerentemente ligada a modelos lineares (como Regressão Linear e Logística), mas seus resultados *podem* ser usados como um passo de pré-processamento (um método filtro sofisticado) para selecionar features *antes* de treinar um modelo não linear.** No entanto, isso vem com ressalvas importantes.\n",
        "\n",
        "Vamos detalhar:\n",
        "\n",
        "1.  **Lasso e Modelos Lineares (Aplicação Direta):**\n",
        "    *   Como vimos, Lasso funciona adicionando uma penalidade `α * Σ|βⱼ|` à função de custo de um modelo linear.\n",
        "    *   Essa penalidade força os coeficientes (`βⱼ`) de features menos importantes (na perspectiva *linear*) a se tornarem exatamente zero.\n",
        "    *   A seleção de features aqui é um **resultado direto** do processo de otimização do *modelo linear regularizado*. As features com `βⱼ ≠ 0` são as \"selecionadas\" por esse modelo.\n",
        "\n",
        "2.  **Lasso como Filtro para Modelos Não Lineares (Aplicação Indireta):**\n",
        "    *   Modelos não lineares (Árvores de Decisão, Random Forests, Gradient Boosting, SVMs não lineares, Redes Neurais) **não possuem coeficientes lineares `βⱼ`** da mesma forma. Eles capturam relações complexas e não lineares de maneiras diferentes (divisões em árvores, transformações de kernel em SVMs, ativações não lineares em redes neurais).\n",
        "    *   Portanto, você **não pode aplicar diretamente** a penalidade L1 da mesma forma durante o treinamento de, por exemplo, um Random Forest.\n",
        "    *   **PORÉM**, você pode usar um modelo linear com Lasso como uma **etapa preliminar de seleção de features**:\n",
        "        *   **Passo 1:** Pegue seu conjunto de dados completo (features `X`, target `y`).\n",
        "        *   **Passo 2:** **Escale** as features `X` (importante para Lasso!).\n",
        "        *   **Passo 3:** Treine um modelo `Lasso` (para regressão) ou `LogisticRegression(penalty='l1')` (para classificação) nos dados escalados. Use `LassoCV` ou `LogisticRegressionCV` para encontrar um bom `α` (ou `C`).\n",
        "        *   **Passo 4:** Identifique as features cujos coeficientes **não foram zerados** pelo modelo L1.\n",
        "        *   **Passo 5:** Crie um novo conjunto de dados `X_selected` contendo **apenas** as features selecionadas no Passo 4.\n",
        "        *   **Passo 6:** Treine seu modelo **não linear** (ex: RandomForestClassifier, XGBoost, etc.) usando `X_selected` e `y`.\n",
        "\n",
        "3.  **Vantagens de Usar Lasso como Filtro para Modelos Não Lineares:**\n",
        "    *   **Redução de Dimensionalidade:** Pode reduzir drasticamente o número de features, especialmente em datasets muito largos (muitas colunas). Isso pode acelerar o treinamento do modelo não linear subsequente.\n",
        "    *   **Remoção de Ruído Linear:** Elimina features que não têm nem mesmo uma relação linear útil com o target, o que *pode* (mas não garante) ajudar o modelo não linear.\n",
        "    *   **Simplicidade Relativa:** Pode ser computacionalmente mais barato do que usar métodos wrapper com o modelo não linear (que exigiriam treinar o modelo não linear muitas vezes).\n",
        "\n",
        "4.  **Desvantagens e Riscos CRUCIAIS:**\n",
        "    *   **Viés Linear na Seleção (O Maior Risco):** Lasso seleciona features com base em sua importância para explicar a variância (ou separar classes) de forma **linear**. Uma feature pode ter uma relação linear fraca (e ser descartada por Lasso), mas ter uma **relação não linear muito forte** que seria perfeitamente capturada por um Random Forest ou Rede Neural. Ao usar Lasso como filtro, **você corre um risco significativo de descartar features que são, na verdade, muito importantes para o modelo não linear.**\n",
        "    *   **Ignora Interações Complexas:** Modelos não lineares são bons em capturar interações entre features (`feature1` só é importante quando `feature2` tem um certo valor). Lasso, sendo linear (sem termos de interação explícitos), geralmente falha em reconhecer a importância de features que só brilham em interações. Você pode descartar uma feature crucial para uma interação importante.\n",
        "    *   **Subconjunto Não Ótimo para o Modelo Não Linear:** O conjunto de features ideal para um modelo linear (Lasso) raramente é o conjunto ideal para um modelo não linear. A seleção feita por Lasso não está otimizada para a forma como o modelo não linear \"vê\" os dados.\n",
        "\n",
        "5.  **Alternativas Melhores para Seleção de Features com Modelos Não Lineares:**\n",
        "    *   **Feature Importance Intrínseca:** Modelos baseados em árvores (Random Forest, Gradient Boosting) calculam suas próprias métricas de importância de feature (ex: redução média de impureza Gini, ou \"feature_importances_\" no scikit-learn). Usar essas importâncias *do próprio modelo não linear* é geralmente mais relevante. (Cuidado: a importância baseada em impureza pode ter vieses).\n",
        "    *   **Permutation Importance:** Uma técnica mais robusta e model-agnóstica. Depois de treinar o modelo não linear, você embaralha aleatoriamente os valores de uma feature e mede o quanto o desempenho do modelo cai. Features que causam uma queda maior são mais importantes *para aquele modelo específico*.\n",
        "    *   **Métodos Wrapper com o Modelo Não Linear:** Usar RFE ou Sequential Feature Selector *tendo o próprio modelo não linear como estimador*. Isso seleciona o subconjunto que funciona melhor *para aquele modelo*, mas é computacionalmente muito caro.\n",
        "    *   **Seleção Baseada em Conhecimento de Domínio:** Muitas vezes a forma mais eficaz.\n",
        "\n",
        "**Conclusão:**\n",
        "\n",
        "Embora tecnicamente possível usar Lasso como um passo de filtragem antes de um modelo não linear, **não é geralmente a abordagem recomendada ou mais eficaz**. Você introduz um forte **viés linear** no processo de seleção, arriscando descartar features que são cruciais para o poder preditivo do modelo não linear.\n",
        "\n",
        "É preferível usar métodos de seleção que estejam mais alinhados com a natureza do modelo não linear que você pretende usar, como as importâncias calculadas pelo próprio modelo, permutation importance, ou wrapper methods (se o custo computacional permitir), combinados com um bom conhecimento do domínio do problema. Lasso como filtro pode ser um último recurso ou um primeiro passo rápido em datasets *extremamente* grandes, mas sempre com a consciência de suas limitações significativas nesse contexto."
      ],
      "metadata": {
        "id": "GeBrgm3JqeUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### E. Vieses na Feature Importance do modelos de Árvore"
      ],
      "metadata": {
        "id": "aBRT9yKBUerU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Você fez uma observação extremamente importante! A afirmação de que as importâncias de features baseadas em árvores (especificamente a métrica padrão **Mean Decrease Impurity - MDI**, também conhecida como \"Gini Importance\" ou \"Feature Importance\" no scikit-learn) podem ser enviesadas é verdadeira e crucial de se entender.\n",
        "\n",
        "Vamos desmembrar o porquê:\n",
        "\n",
        "**1. Como Funciona a Importância Baseada em Impureza (MDI)?**\n",
        "\n",
        "*   **Construção da Árvore:** Árvores de decisão (e, por extensão, florestas como Random Forest e Gradient Boosting) constroem-se fazendo divisões (splits) nos nós. A cada nó, o algoritmo avalia várias features e vários pontos de divisão (ou categorias) para encontrar a divisão que resulta na **maior redução de impureza** (medida por Gini ou Entropia) nos nós filhos resultantes.\n",
        "*   **Cálculo da Importância:** A importância de uma feature (segundo MDI) é calculada como a **soma total da redução de impureza** proporcionada por todas as divisões feitas *naquela feature específica*, em *todas as árvores* da floresta. Geralmente, isso é normalizado para que a soma das importâncias de todas as features seja 1.\n",
        "*   **Intuição:** Features que são usadas com frequência, especialmente no topo da árvore (afetando mais amostras), e que levam a divisões \"puras\" (grande redução de impureza), recebem uma pontuação de importância mais alta.\n",
        "\n",
        "**2. O Viés para Features Numéricas (Contínuas)**\n",
        "\n",
        "*   **Mais Oportunidades de Divisão:** Para uma feature numérica, uma árvore pode testar *muitos* pontos de divisão potenciais (thresholds). Por exemplo, para a feature \"Idade\", ela pode testar `Idade < 25`, `Idade < 30`, `Idade < 30.5`, etc.\n",
        "*   **Comparação com Categóricas de Baixa Cardinalidade:** Para uma feature categórica com poucas categorias (ex: 'Gênero' com 'Masculino', 'Feminino', após One-Hot Encoding, talvez 'Gênero_Masculino = 0/1'), há apenas *um* ponto de divisão óbvio por coluna OHE. Mesmo com suporte nativo a categóricas, o número de *maneiras* de dividir 2 ou 3 categorias é muito limitado comparado às dezenas ou centenas de limiares testáveis para uma variável numérica.\n",
        "*   **O Resultado do Viés:** Como a feature numérica oferece muito mais \"chances\" (pontos de divisão) de ser selecionada em cada nó, ela tem uma probabilidade maior de ser escolhida para uma divisão, mesmo que seu poder preditivo real não seja superior ao de uma feature categórica. O algoritmo pode encontrar um limiar que, por acaso ou devido a flutuações nos dados de treino, produz uma boa redução de impureza *naquele momento*. Essa maior frequência de seleção infla artificialmente sua pontuação total de MDI.\n",
        "\n",
        "**3. O Viés para Features Categóricas de Alta Cardinalidade**\n",
        "\n",
        "*   **O Problema da Cardinalidade:** Considere uma feature como 'CEP' (Zip Code) ou 'ID de Usuário', que pode ter milhares de valores únicos.\n",
        "*   **Muitas Opções de Particionamento:** Mesmo com tratamento categórico inteligente (como em LightGBM ou CatBoost, que agrupam categorias), uma feature com muitas categorias oferece um número vasto de maneiras de particionar os dados. A árvore pode testar divisões como \"CEP está em {10001, 10002, 10005} vs. outros\", ou \"ID de Usuário < 5000 vs. >= 5000\" (se tratada numericamente após encoding inadequado), ou outras combinações.\n",
        "*   **Facilidade de Overfitting Local:** Com tantas opções de divisão, é mais fácil para o algoritmo encontrar uma divisão (baseada em categorias específicas ou grupos delas) que **separa perfeitamente um pequeno subconjunto dos dados de *treinamento***, puramente por acaso. Isso levará a uma grande redução de impureza *para aquela divisão específica* no conjunto de treino.\n",
        "*   **O Resultado do Viés:** Essas grandes (mas potencialmente espúrias e não generalizáveis) reduções de impureza obtidas no treino são somadas, inflando a pontuação MDI da feature de alta cardinalidade. Essencialmente, features com alta cardinalidade têm mais flexibilidade para \"memorizar\" ou se ajustar ao ruído específico dos dados de treinamento.\n",
        "\n",
        "**Por Que Isso Acontece Fundamentalmente?**\n",
        "\n",
        "O MDI reflete o quão útil uma feature foi para **ajustar o modelo aos dados de *treinamento***. Ele mede a qualidade das divisões *na amostra usada para construir as árvores*. Features que oferecem mais flexibilidade de divisão (numéricas e de alta cardinalidade) têm mais chances de encontrar divisões que *parecem* boas nos dados de treinamento, mesmo que essas divisões não generalizem bem para dados não vistos.\n",
        "\n",
        "**Consequências do Viés:**\n",
        "\n",
        "*   Você pode superestimar a importância de features numéricas ou de alta cardinalidade.\n",
        "*   Você pode subestimar a importância de features categóricas de baixa cardinalidade que são genuinamente preditivas.\n",
        "*   Sua seleção de features baseada puramente em MDI pode ser enganosa.\n",
        "\n",
        "**Soluções e Alternativas Mais Robustas:**\n",
        "\n",
        "1.  **Permutation Importance (Importância por Permutação):**\n",
        "    *   **Como Funciona:** Depois de treinar o modelo, pegue um conjunto de dados separado (validação ou teste). Meça o desempenho do modelo (ex: Acurácia, AUC, R²). Em seguida, embaralhe aleatoriamente os valores de *uma única feature* nesse conjunto de dados e remeça o desempenho. A queda no desempenho indica a importância daquela feature.\n",
        "    *   **Por que é Melhor:** Mede diretamente o quanto o modelo *depende* da feature para fazer previsões corretas em dados *não vistos*. Não depende de como a árvore foi construída ou da impureza, eliminando o viés de cardinalidade/tipo. É considerada a abordagem mais confiável.\n",
        "    *   **Desvantagem:** Computacionalmente mais cara, pois exige múltiplas reavaliações do modelo.\n",
        "\n",
        "2.  **SHAP Values (SHapley Additive exPlanations):**\n",
        "    *   Baseado em teoria dos jogos (valores de Shapley), atribui a cada feature a contribuição média para a previsão de uma instância específica, considerando todas as combinações possíveis de features.\n",
        "    *   Fornece importâncias globais (média dos valores SHAP absolutos) e locais (para previsões individuais). Geralmente menos enviesado que MDI.\n",
        "    *   **Desvantagem:** Pode ser computacionalmente intensivo, especialmente para alguns algoritmos.\n",
        "\n",
        "3.  **Cuidado com a Codificação:** Para alta cardinalidade, evite Label Encoding. Use Target Encoding (com cuidado para evitar leakage), Hashing, ou Embedding Layers (em redes neurais).\n",
        "\n",
        "4.  **Use MDI com Cautela:** Esteja ciente do viés. Não confie cegamente nos rankings de MDI, especialmente ao comparar features de tipos ou cardinalidades diferentes. Use-o como uma primeira indicação, mas valide com Permutation Importance ou conhecimento de domínio.\n",
        "\n",
        "Em resumo, o MDI é uma métrica conveniente e rápida, mas sua tendência a favorecer features numéricas e de alta cardinalidade devido à maior flexibilidade de divisão no *treinamento* a torna menos confiável para comparações justas. Para uma avaliação mais robusta da importância preditiva real, **Permutation Importance** é geralmente a escolha preferida."
      ],
      "metadata": {
        "id": "VUVTDxx-Ule1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### F. Importância por Permutação (Permutation Importance)"
      ],
      "metadata": {
        "id": "C-58BUD4qrUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, vamos fazer um \"Deep Dive\" na **Permutation Importance (Importância por Permutação)**. Esta é uma técnica poderosa e cada vez mais popular para avaliar a importância das features, superando muitas das limitações dos métodos baseados em impureza (MDI), como os discutidos anteriormente.\n",
        "\n",
        "**1. O Que é Permutation Importance? (Motivação e Objetivo)**\n",
        "\n",
        "*   **Motivação:** As métricas de importância baseadas em impureza (MDI), como a padrão do scikit-learn para Random Forests, são calculadas com base em como as features contribuíram para a redução da impureza *durante o treinamento*. Isso tem desvantagens:\n",
        "    *   Tendem a inflar a importância de features numéricas e categóricas de alta cardinalidade.\n",
        "    *   Refletem o desempenho no conjunto de *treinamento*, não necessariamente a capacidade de generalização.\n",
        "    *   Não são facilmente aplicáveis a todos os tipos de modelos.\n",
        "*   **Objetivo da Permutation Importance:** Medir a importância de uma feature avaliando o quanto o **desempenho do modelo (já treinado)** *cai* quando a informação contida naquela feature é \"quebrada\" ou tornada aleatória. A ideia é: se uma feature é importante, embaralhar seus valores deve prejudicar significativamente a performance do modelo; se for inútil, embaralhar não fará muita diferença.\n",
        "*   **Foco na Predição:** Diferente do MDI, a Permutation Importance foca no impacto da feature nas **previsões** do modelo em um conjunto de dados específico (idealmente, um conjunto não visto durante o treinamento, como validação ou teste).\n",
        "\n",
        "**2. Como Funciona? (O Algoritmo Detalhado)**\n",
        "\n",
        "O processo é conceitualmente simples, mas poderoso:\n",
        "\n",
        "*   **Passo 1: Treinar o Modelo:** Treine *qualquer* modelo de aprendizado de máquina (Random Forest, Regressão Logística, SVM, XGBoost, Rede Neural, etc.) da maneira usual, usando seu conjunto de treinamento. Você precisa de um modelo **já ajustado (`fitted`)**.\n",
        "\n",
        "*   **Passo 2: Escolher Dados e Métrica:**\n",
        "    *   Selecione um conjunto de dados para avaliação. **Idealmente, use um conjunto de validação ou teste** que não foi usado para treinar o modelo. Isso mede a importância da feature para a *generalização*. (Usar o conjunto de treino também é possível, mas mede a importância para o ajuste no treino, similar ao MDI).\n",
        "    *   Escolha uma métrica de desempenho apropriada para o seu problema (ex: Acurácia, AUC, F1-score para classificação; R², Erro Médio Absoluto (MAE), Erro Quadrático Médio (MSE) para regressão).\n",
        "\n",
        "*   **Passo 3: Calcular o Desempenho Base:** Faça previsões com o modelo treinado no conjunto de dados escolhido (sem nenhuma modificação) e calcule a métrica de desempenho. Este é o seu **score base** ou de referência.\n",
        "\n",
        "*   **Passo 4: Iterar Sobre Cada Feature:** Para cada feature `j` no conjunto de dados:\n",
        "    *   **a. Embaralhar (Permutar):** Crie uma cópia temporária do conjunto de dados. Nesta cópia, **embaralhe aleatoriamente os valores *apenas* da coluna correspondente à feature `j`**. Isso preserva a distribuição marginal da feature (os mesmos valores ainda estão lá, apenas em ordens diferentes), mas quebra a relação entre essa feature e a variável alvo (e com as outras features) *naquele conjunto de dados específico*.\n",
        "    *   **b. Fazer Previsões:** Use o **mesmo modelo treinado** (do Passo 1) para fazer previsões neste conjunto de dados com a feature `j` embaralhada.\n",
        "    *   **c. Calcular o Desempenho Pós-Embaralhamento:** Calcule a métrica de desempenho usando essas novas previsões.\n",
        "    *   **d. Calcular a Importância da Feature `j`:** A importância da feature `j` é a **diferença** entre o score base e o score pós-embaralhamento:\n",
        "        `Importância(j) = Score_Base - Score_Embaralhado(j)`\n",
        "        Alternativamente, pode-se usar a razão `Score_Base / Score_Embaralhado(j)`. Uma queda maior no score significa maior importância.\n",
        "    *   **(Crucial): Descarte a cópia embaralhada ou reverta o embaralhamento antes de passar para a próxima feature.**\n",
        "\n",
        "*   **Passo 5 (Opcional, Mas Altamente Recomendado): Repetir o Embaralhamento:** O processo de embaralhamento introduz aleatoriedade. Para obter uma estimativa mais estável da importância, repita o Passo 4 (embaralhar, prever, calcular score) **várias vezes (`n_repeats`)** para cada feature, usando um embaralhamento diferente a cada vez. A importância final da feature será a **média** (e pode-se também calcular o desvio padrão) das quedas de score observadas nas múltiplas repetições.\n",
        "\n",
        "*   **Passo 6: Rankear as Features:** Ordene as features em ordem decrescente de sua importância calculada (a média da queda no score).\n",
        "\n",
        "**3. Por Que Funciona? (A Lógica)**\n",
        "\n",
        "*   **Quebrando a Ligação:** Ao embaralhar uma única coluna (feature), você destrói a capacidade do modelo de usar a informação *real* daquela feature para fazer a previsão correta para cada instância *naquele conjunto de dados*. O modelo ainda \"vê\" valores plausíveis (pois a distribuição da feature é mantida), mas eles não correspondem mais à instância correta.\n",
        "*   **Medindo a Dependência:** Se o modelo aprendeu a depender fortemente daquela feature durante o treinamento, suas previsões ficarão significativamente piores quando essa dependência for quebrada pelo embaralhamento. A magnitude da queda no desempenho quantifica diretamente essa dependência.\n",
        "*   **Foco no Modelo Treinado:** A técnica avalia o que o *modelo específico que você treinou* aprendeu a fazer, e não alguma propriedade intrínseca teórica dos dados.\n",
        "\n",
        "**4. Como Usar? (Implementação com `scikit-learn`)**\n",
        "\n",
        "O scikit-learn oferece uma implementação direta e eficiente: `sklearn.inspection.permutation_importance`.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier # Exemplo de modelo\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import accuracy_score # Exemplo de métrica\n",
        "\n",
        "# Suponha que X, y já existem\n",
        "# X pode ser um DataFrame Pandas para facilitar a leitura dos nomes das features\n",
        "\n",
        "# 1. Dividir dados (TREINO e VALIDAÇÃO/TESTE)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Treinar o modelo\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 3. Calcular Permutation Importance no conjunto de VALIDAÇÃO\n",
        "#    scoring: nome da métrica (ver sklearn.metrics.SCORERS.keys()) ou uma função callable\n",
        "#    n_repeats: número de vezes para embaralhar cada feature (crucial!)\n",
        "#    random_state: para reprodutibilidade do embaralhamento\n",
        "#    n_jobs: para paralelizar (acelera se tiver múltiplos núcleos)\n",
        "result = permutation_importance(\n",
        "    estimator=model,\n",
        "    X=X_val,\n",
        "    y=y_val,\n",
        "    scoring='accuracy', # Ou 'roc_auc', 'f1', 'r2', 'neg_mean_absolute_error', etc.\n",
        "    n_repeats=10,       # Repetir 10 vezes para estabilidade\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 4. Organizar e Visualizar os Resultados\n",
        "importances_mean = result.importances_mean\n",
        "importances_std = result.importances_std\n",
        "feature_names = X_val.columns if isinstance(X_val, pd.DataFrame) else [f'Feature {i}' for i in range(X_val.shape[1])]\n",
        "\n",
        "# Criar um DataFrame para visualização\n",
        "perm_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance Mean': importances_mean,\n",
        "    'Importance Std': importances_std\n",
        "}).sort_values(by='Importance Mean', ascending=False)\n",
        "\n",
        "print(\"Permutation Importance (Validation Set):\")\n",
        "print(perm_importance_df)\n",
        "\n",
        "# Plotar as importâncias\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(perm_importance_df['Feature'], perm_importance_df['Importance Mean'],\n",
        "        yerr=perm_importance_df['Importance Std'], capsize=5)\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Importância Média (Queda na Acurácia)')\n",
        "plt.title('Importância por Permutação (com Desvio Padrão das Repetições)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**5. Vantagens da Permutation Importance:**\n",
        "\n",
        "*   **Model-Agnostic:** Funciona com qualquer modelo ajustado (preto ou branco).\n",
        "*   **Foco na Generalização:** Ao usar dados de validação/teste, mede a importância para o desempenho em dados não vistos.\n",
        "*   **Menos Enviesada:** Não sofre dos mesmos vieses de cardinalidade/tipo de feature que o MDI.\n",
        "*   **Captura Interações:** O impacto de embaralhar uma feature inclui a perda de suas interações com outras features, pois essas interações também são quebradas.\n",
        "*   **Intuitiva:** O conceito de \"queda no desempenho ao quebrar a feature\" é fácil de entender.\n",
        "*   **Não Requer Retreinamento:** É calculado após o modelo já estar treinado, sendo mais rápido que métodos Wrapper.\n",
        "\n",
        "**6. Limitações e Desvantagens:**\n",
        "\n",
        "*   **Custo Computacional:** Pode ser lento se:\n",
        "    *   O conjunto de dados de avaliação for grande.\n",
        "    *   O modelo levar muito tempo para fazer previsões.\n",
        "    *   O número de features for alto.\n",
        "    *   `n_repeats` for alto (mas necessário para estabilidade).\n",
        "*   **Problema com Features Altamente Correlacionadas:** Esta é a **principal ressalva**. Se duas features (X1, X2) são fortemente correlacionadas e ambas são importantes, embaralhar apenas X1 pode não diminuir muito o desempenho, porque o modelo ainda pode obter quase a mesma informação de X2 (que não foi embaralhada). Isso pode fazer com que a importância de *ambas* as features correlacionadas seja subestimada. O MDI, por outro lado, poderia atribuir alta importância a ambas ou a uma delas arbitrariamente.\n",
        "*   **Dependência da Métrica:** A importância calculada é relativa à métrica de `scoring` escolhida.\n",
        "*   **Aleatoriedade:** Requer `n_repeats` > 1 para resultados estáveis.\n",
        "\n",
        "**7. Considerações Práticas e Dicas:**\n",
        "\n",
        "*   **Use Dados de Validação/Teste:** Para a avaliação mais significativa da importância para generalização.\n",
        "*   **Escolha `n_repeats` Adequado:** Comece com 5 ou 10. Se o desvio padrão (`importances_std`) for muito alto em relação à média (`importances_mean`) para features importantes, aumente `n_repeats`.\n",
        "*   **Cuidado com Correlações:** Antes de calcular, verifique a matriz de correlação das suas features. Se houver grupos altamente correlacionados, interprete suas importâncias com cautela. Considere:\n",
        "    *   Analisar a importância conjunta do grupo (difícil de fazer diretamente com PI).\n",
        "    *   Remover uma das features correlacionadas (baseado em conhecimento de domínio ou outra métrica) antes de calcular a importância das restantes.\n",
        "    *   Usar técnicas que lidam melhor com colinearidade (como PCA antes da modelagem, ou Elastic Net para modelos lineares).\n",
        "*   **Importância Negativa:** É possível obter uma importância média ligeiramente negativa. Isso geralmente significa que a feature tem importância próxima de zero, e as flutuações aleatórias do embaralhamento fizeram o score *melhorar* por acaso em algumas repetições. Considere essas features como não importantes.\n",
        "\n",
        "**Conclusão:**\n",
        "\n",
        "Permutation Importance é uma técnica robusta e confiável para avaliar a importância das features, superando as principais desvantagens do MDI. Sua natureza model-agnóstica e seu foco no desempenho preditivo em dados potencialmente não vistos a tornam uma ferramenta valiosa para entender quais features realmente impulsionam as previsões do seu modelo e para construir confiança nos resultados. Embora tenha um custo computacional maior e requeira atenção especial a features correlacionadas, **seus benefícios em termos de confiabilidade geralmente superam essas desvantagens, tornando-a a escolha preferida em muitos cenários práticos para avaliação de feature importance.**"
      ],
      "metadata": {
        "id": "6CFF0-clq5bo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####  **Dúvida 1:** O método de Permutation Importance se encaixa como um método de filtragem, empacotamento ou incorporado?"
      ],
      "metadata": {
        "id": "kU9xRlqKvNcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta é uma excelente questão porque Permutation Importance (PI) **não se encaixa perfeitamente em nenhuma dessas três categorias clássicas**, embora compartilhe características com elas. Veja porquê:\n",
        "\n",
        "*   **Não é Filtro (Filter):** Métodos filtro avaliam features com base em suas propriedades estatísticas intrínsecas (variância, correlação com o target, informação mútua) **independentemente de qualquer modelo**. PI, por outro lado, **depende fundamentalmente de um modelo já treinado** para medir a queda no desempenho.\n",
        "*   **Não é Incorporado (Embedded):** Métodos incorporados realizam a seleção de features como parte **intrínseca do processo de treinamento** do modelo (ex: Lasso zerando coeficientes durante o `fit`, ou árvores calculando MDI enquanto se constroem). PI é calculado **após** o modelo já ter sido completamente treinado (é um método *post-hoc*).\n",
        "*   **Não é (Exatamente) Empacotamento (Wrapper):** Métodos wrapper usam um modelo para avaliar e selecionar **subconjuntos** de features (ex: RFE que treina repetidamente em subconjuntos menores, ou SFS que adiciona/remove features uma a uma). PI **não seleciona um subconjunto ótimo** iterativamente; ele fornece uma **pontuação de importância para cada feature individualmente**, com base em como sua permutação afeta o desempenho do modelo *completo* (com todas as features originais, exceto a permutada).\n",
        "\n",
        "**Então, onde PI se encaixa?**\n",
        "\n",
        "A melhor forma de classificar Permutation Importance é como um método **Model-Based (Baseado em Modelo) para Avaliação de Importância de Features**.\n",
        "\n",
        "*   É **baseado em modelo** porque requer um modelo treinado.\n",
        "*   É para **avaliação de importância**, pois seu resultado principal é um ranking ou score para cada feature.\n",
        "*   **Pode ser Usado para Seleção (como um filtro pós-modelo):** Embora PI não *seja* um método de seleção em si, seus resultados (o ranking de importância) são frequentemente *usados* para informar uma decisão de seleção de features. Você pode, por exemplo, calcular a PI e depois decidir manter apenas as N features mais importantes ou aquelas acima de um certo limiar de importância. Nesse sentido, quando usado *para selecionar* features, ele age como um **filtro sofisticado**, onde o critério de filtragem é derivado do impacto da feature no desempenho de um modelo específico, em vez de uma estatística pura."
      ],
      "metadata": {
        "id": "YhBEC1k9u5BS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####  **Dúvida 2:** É possível utilizar Permutation Importance com cross validation?"
      ],
      "metadata": {
        "id": "W-cFMeNjvJ9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sim, é absolutamente possível e altamente recomendado!** Usar Permutation Importance dentro de um esquema de validação cruzada (Cross-Validation - CV) fornece uma estimativa muito mais robusta e confiável da importância das features, menos suscetível às particularidades de uma única divisão treino-teste/validação.\n",
        "\n",
        "**Como funciona:**\n",
        "\n",
        "1.  **Configurar a Validação Cruzada:** Divida seus dados de treinamento em K folds (por exemplo, 5 ou 10 folds usando `KFold` ou `StratifiedKFold`).\n",
        "2.  **Iterar pelos Folds:** Para cada fold `k` (de 1 a K):\n",
        "    *   **Treinar o Modelo:** Use os `K-1` folds restantes como dados de treinamento para ajustar (`fit`) o seu modelo.\n",
        "    *   **Avaliar a Permutation Importance:** Use o **fold `k` (que foi deixado de fora)** como o conjunto de dados de avaliação (`X_val`, `y_val`). Calcule a Permutation Importance (com `n_repeats` adequado) usando o modelo treinado nos outros folds e avaliando neste fold `k`.\n",
        "    *   **Armazenar Resultados:** Guarde os scores de importância calculados para cada feature *neste fold específico*.\n",
        "3.  **Agregar Resultados:** Após iterar por todos os K folds, você terá K estimativas de importância para cada feature. A importância final de cada feature pode ser calculada como:\n",
        "    *   **Média:** A média das importâncias obtidas nos K folds (esta é a estimativa mais comum da importância \"geral\").\n",
        "    *   **Desvio Padrão:** O desvio padrão das importâncias obtidas nos K folds (isso dá uma medida da **estabilidade** da importância da feature; se o desvio padrão for alto em relação à média, a importância varia muito dependendo do subconjunto de dados usado para treinar/avaliar).\n",
        "\n",
        "**Vantagens de usar PI com CV:**\n",
        "\n",
        "*   **Robustez:** Reduz a dependência de uma única divisão de dados, dando uma ideia melhor da importância \"média\" da feature.\n",
        "*   **Estabilidade:** O desvio padrão entre os folds informa sobre a consistência da importância da feature. Features consistentemente importantes terão baixo desvio padrão.\n",
        "*   **Melhor Estimativa de Generalização:** A avaliação é sempre feita em dados que não foram usados para treinar o modelo *daquele fold específico*, aproximando melhor o desempenho em dados futuros.\n",
        "\n",
        "**Desvantagem:**\n",
        "\n",
        "*   **Custo Computacional:** Calcular a Permutation Importance já pode ser caro (especialmente com muitos `n_repeats`). Fazer isso K vezes dentro de um loop de CV aumenta significativamente o tempo de execução.\n",
        "\n",
        "**Implementação:** Você precisaria escrever um loop de CV manualmente e chamar `permutation_importance` dentro dele para cada fold, armazenando e depois agregando os resultados. Não há (até o momento) uma função única no scikit-learn que faça PI com CV automaticamente da mesma forma que `cross_val_score` faz para métricas de desempenho, mas a lógica é direta de implementar.\n",
        "\n",
        "Em resumo, usar Permutation Importance com validação cruzada é a prática recomendada para obter as estimativas mais confiáveis e estáveis da importância das features, apesar do custo computacional adicional."
      ],
      "metadata": {
        "id": "BupL8Uuhu1FU"
      }
    }
  ]
}